Task_Index,Task,Datasets_Retrieved,Prompt,Manually_Chosen_Dataset,Manually_Chosen_Config,Reranker_Dataset,Reranker_Config
task001: question generation from entity tracking,"In this task, you're given passages that contain mentions of names of people, places, or things. Some of these mentions refer to the same person, place, or thing. Your job is to write questions that evaluate one's understanding of such references. Good questions are expected to link pronouns (she, her, him, his, their, etc.) or other mentions to people, places, or things to which they may refer. Do not ask questions that can be answered correctly without understanding the paragraph or having multiple answers. Avoid questions that do not link phrases referring to the same entity. For each of your questions, the answer should be one or more phrases in the paragraph, and it should be unambiguous.{
            ""input"": ""Passage: Nearing London, Oliver encounters Jack Dawkins, a pickpocket more commonly known by the nickname the \""Artful Dodger\"", and his sidekick, a boy of a humorous nature named Charley Bates, but Oliver's innocent and trusting nature fails to see any dishonesty in their actions. The Dodger provides Oliver with a free meal and tells him of a gentleman in London who will \""give him lodgings for nothing, and never ask for change\"". Grateful for the unexpected assistance, Oliver follows the Dodger to the \""old gentleman's\"" residence. In this way Oliver unwittingly falls in with an infamous Jewish criminal known as Fagin, the gentleman of whom the Artful Dodger spoke. Ensnared, Oliver lives with Fagin and his gang of juvenile pickpockets in their lair at Saffron Hill for some time, unaware of their criminal occupations. He believes they make wallets and handkerchiefs."",
            ""output"": ""Who believes Fagin's gang make wallets and handkerchiefs?."",
            ""explanation"": ""This question is based on the following sentence in the passage \""He believes they make wallets and handkerchiefs\"". It evaluates the understanding that the pronoun \""he\"" refers to name \""Oliver\"". You can ask questions like this one about most pronouns in a paragraph.""
        },
        {
            ""input"": ""Passage: Nearing London, Oliver encounters Jack Dawkins, a pickpocket more commonly known by the nickname the \""Artful Dodger\"", and his sidekick, a boy of a humorous nature named Charley Bates, but Oliver's innocent and trusting nature fails to see any dishonesty in their actions. The Dodger provides Oliver with a free meal and tells him of a gentleman in London who will \""give him lodgings for nothing, and never ask for change\"". Grateful for the unexpected assistance, Oliver follows the Dodger to the \""old gentleman's\"" residence. In this way Oliver unwittingly falls in with an infamous Jewish criminal known as Fagin, the gentleman of whom the Artful Dodger spoke. Ensnared, Oliver lives with Fagin and his gang of juvenile pickpockets in their lair at Saffron Hill for some time, unaware of their criminal occupations. He believes they make wallets and handkerchiefs."",
            ""output"": ""What is the alias of the person whose sidekick had a humorous nature?."",
            ""explanation"": ""This question is based on the following sentence in the passage \""Nearing London, Oliver encounters Jack Dawkins, a pickpocket more commonly known by the nickname the \""Artful Dodger\"", and his sidekick, a boy of a humorous nature named Charley Bates\"". The pronoun \""his\"" refers to a person with multiple names. But since the question explicitly asks for the alias, the answer is unambiguous.""
        },",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 In this task, you're given passages that contain mentions of names of people, places, or things. Some of these mentions refer to the same person, place, or thing. Your job is to write questions that evaluate one's understanding of such references. Good questions are expected to link pronouns (she, her, him, his, their, etc.) or other mentions to people, places, or things to which they may refer. Do not ask questions that can be answered correctly without understanding the paragraph or having multiple answers. Avoid questions that do not link phrases referring to the same entity. For each of your questions, the answer should be one or more phrases in the paragraph, and it should be unambiguous. and these are some examples of the same: Passage: Nearing London, Oliver encounters Jack Dawkins, a pickpocket more commonly known by the nickname the ""Artful Dodger"", and his sidekick, a boy of a humorous nature named Charley Bates, but Oliver's innocent and trusting nature fails to see any dishonesty in their actions. The Dodger provides Oliver with a free meal and tells him of a gentleman in London who will ""give him lodgings for nothing, and never ask for change"". Grateful for the unexpected assistance, Oliver follows the Dodger to the ""old gentleman's"" residence. In this way Oliver unwittingly falls in with an infamous Jewish criminal known as Fagin, the gentleman of whom the Artful Dodger spoke. Ensnared, Oliver lives with Fagin and his gang of juvenile pickpockets in their lair at Saffron Hill for some time, unaware of their criminal occupations. He believes they make wallets and handkerchiefs.

Output: Who believes Fagin's gang make wallets and handkerchiefs?.
Explanation: This question is based on the following sentence in the passage ""He believes they make wallets and handkerchiefs"". It evaluates the understanding that the pronoun ""he"" refers to name ""Oliver"". You can ask questions like this one about most pronouns in a paragraph.

Passage: Nearing London, Oliver encounters Jack Dawkins, a pickpocket more commonly known by the nickname the ""Artful Dodger"", and his sidekick, a boy of a humorous nature named Charley Bates, but Oliver's innocent and trusting nature fails to see any dishonesty in their actions. The Dodger provides Oliver with a free meal and tells him of a gentleman in London who will ""give him lodgings for nothing, and never ask for change"". Grateful for the unexpected assistance, Oliver follows the Dodger to the ""old gentleman's"" residence. In this way Oliver unwittingly falls in with an infamous Jewish criminal known as Fagin, the gentleman of whom the Artful Dodger spoke. Ensnared, Oliver lives with Fagin and his gang of juvenile pickpockets in their lair at Saffron Hill for some time, unaware of their criminal occupations. He believes they make wallets and handkerchiefs.

Output: What is the alias of the person whose sidekick had a humorous nature?.
Explanation: This question is based on the following sentence in the passage ""Nearing London, Oliver encounters Jack Dawkins, a pickpocket more commonly known by the nickname the ""Artful Dodger"", and his sidekick, a boy of a humorous nature named Charley Bates"". The pronoun ""his"" refers to a person with multiple names. But since the question explicitly asks for the alias, the answer is unambiguous. 

There are 13 datasets available for this task, each indicated by number identifier []. 

[1] facebook/babi_qa
: Description-The (20) QA bAbI tasks are a set of proxy tasks that evaluate reading
comprehension via question answering. Our tasks measure understanding
in several ways: whether a system is able to answer questions via chaining facts,
simple induction, deduction and many more. The tasks are designed to be prerequisites
for any system that aims to be capable of conversing with a human.
The aim is to classify these tasks into skill sets,so that researchers
can identify (and then rectify)the failings of their systems.
.
. This dataset has the following configs:
  

	[b] en-10k-qa1
: The columns in this config are story_id, story_type, story_text, story_supporting_ids, story_answer.
 An example row from this config is {""story.id"": ""[\""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"", \""10\""..."", ""story.type"": ""[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1]"", ""story.text"": ""[\""Mary moved to the bathroom.\"", \""John went to the ..."", ""story.supporting_ids"": ""[[], [], [\""1\""], [], [], [\""4\""], [], [], [\""4\""], [], ..."", ""story.answer"": ""[\""\"", \""\"", \""bathroom\"", \""\"", \""\"", \""hallway\"", \""\"", \""\"", \""h...""}.

 

	[c] hn-10k-qa1
: The columns in this config are story_id, story_type, story_text, story_supporting_ids, story_answer.
 An example row from this config is {""story.id"": ""[\""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"", \""10\""..."", ""story.type"": ""[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1]"", ""story.text"": ""[\""Sita gusalkhaney mein gayi.\"", \""Priya sayanakaksh..."", ""story.supporting_ids"": ""[[], [], [\""2\""], [], [], [\""5\""], [], [], [\""7\""], [], ..."", ""story.answer"": ""[\""\"", \""\"", \""sayanakaksh\"", \""\"", \""\"", \""rasoi ghar\"", \""\"", ...""}.

 

	[d] en-valid-qa1
: The columns in this config are story_id, story_type, story_text, story_supporting_ids, story_answer.
 An example row from this config is {""story.id"": ""[\""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"", \""10\""..."", ""story.type"": ""[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1]"", ""story.text"": ""[\""Mary moved to the bathroom.\"", \""John went to the ..."", ""story.supporting_ids"": ""[[], [], [\""1\""], [], [], [\""4\""], [], [], [\""4\""], [], ..."", ""story.answer"": ""[\""\"", \""\"", \""bathroom\"", \""\"", \""\"", \""hallway\"", \""\"", \""\"", \""h...""}.

 

	[e] shuffled-10k-qa1
: The columns in this config are story_id, story_type, story_text, story_supporting_ids, story_answer.
 An example row from this config is {""story.id"": ""[\""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"", \""10\""..."", ""story.type"": ""[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1]"", ""story.text"": ""[\""Utxi ybnha qb qzh ptqzxbby.\"", \""Hbzm jhmq qb qzh ..."", ""story.supporting_ids"": ""[[], [], [\""1\""], [], [], [\""4\""], [], [], [\""4\""], [], ..."", ""story.answer"": ""[\""\"", \""\"", \""ptqzxbby\"", \""\"", \""\"", \""ztuujti\"", \""\"", \""\"", \""z...""}.

 

	[f] en-qa1
: The columns in this config are story_id, story_type, story_text, story_supporting_ids, story_answer.
 An example row from this config is {""story.id"": ""[\""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"", \""10\""..."", ""story.type"": ""[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1]"", ""story.text"": ""[\""Mary moved to the bathroom.\"", \""John went to the ..."", ""story.supporting_ids"": ""[[], [], [\""1\""], [], [], [\""4\""], [], [], [\""4\""], [], ..."", ""story.answer"": ""[\""\"", \""\"", \""bathroom\"", \""\"", \""\"", \""hallway\"", \""\"", \""\"", \""h...""}.

 




[2] drop
: Description-DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.
. DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a
question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or
 sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was
 necessary for prior datasets.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are section_id, query_id, passage, question, answers_spans_spans, answers_spans_types.
 An example row from this config is {""section_id"": ""\""nfl_2201\"""", ""query_id"": ""\""f16c0ee7-f131-4a8b-a6ac-4d275ea68066\"""", ""passage"": ""\""To start the season, the Lions traveled south to ..."", ""question"": ""\""How many points did the buccaneers need to tie in..."", ""answers_spans.spans"": ""[\""3\""]"", ""answers_spans.types"": ""[\""number\""]""}.

 




[3] SajjadAyoubi/persian_qa
: Description-\\\\
Persian Question Answering (PersianQA) Dataset is a reading comprehension dataset on Persian Wikipedia. 
The crowd-sourced dataset consists of more than 9,000 entries. Each entry can be either an impossible to answer or a question with one or more answers spanning in the passage (the context) from which the questioner proposed the question. Much like the SQuAD2.0 dataset, the impossible or unanswerable questions can be utilized to create a system which ""knows that it doesn't know the answer"".
.
. This dataset has the following configs:
  

	[b] persian_qa
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""1"", ""title"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""context"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""question"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""answers.text"": ""[\""\\u062f\\u0631 \\u0634\\u0631\\u0642 \\u0634\\u0647\\u06..."", ""answers.answer_start"": ""[114]""}.

 




[4] squad_v2
: Description-combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers
 to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but
 also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] squad_v2
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""56be85543aeaaa14008c9063\"""", ""title"": ""\""Beyonc\\u00e9\"""", ""context"": ""\""Beyonc\\u00e9 Giselle Knowles-Carter (/bi\\u02d0\\u0..."", ""question"": ""\""When did Beyonce start becoming popular?\"""", ""answers.text"": ""[\""in the late 1990s\""]"", ""answers.answer_start"": ""[269]""}.

 




[5] shivmoha/squad-unanswerable
: Description-combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers
 to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but
 also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] squad_v2
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5a8d7bf7df8bba001a0f9ab1\"""", ""title"": ""\""The_Legend_of_Zelda:_Twilight_Princess\"""", ""context"": ""\""The Legend of Zelda: Twilight Princess (Japanese:..."", ""question"": ""\""What category of game is Legend of Zelda: Austral..."", ""answers.text"": ""[]"", ""answers.answer_start"": ""[]""}.

 




[6] TurkuNLP/squad_v2_fi
: Description-combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers
 to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but
 also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] squad_v2_fi
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""56be85543aeaaa14008c9063\"""", ""title"": ""\""Beyonc\\u00e9\"""", ""context"": ""\""Beyonc\\u00e9 Giselle Knowles-Carter (/bi\\u02d0\\u0..."", ""question"": ""\""Milloin Beyoncesta alkoi tulla suosittu?\"""", ""answers.text"": ""[\""1990-luvun lopulla\""]"", ""answers.answer_start"": ""[274]""}.

 




[7] GEM/squad_v2
: Description- SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers
 to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but
 also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] gem_data_split
: The columns in this config are gem_id, id, title, context, question, target, references, answers_text, answers_answer_start.
 An example row from this config is {""gem_id"": ""\""gem-squad_v2-train-0\"""", ""id"": ""\""56f89ee99b226e1400dd0cd5\"""", ""title"": ""\""Guinea-Bissau\"""", ""context"": ""\""Guinea-Bissau (i/\\u02c8\\u0261\\u026ani b\\u026a\\u02..."", ""question"": ""\""What is the official name for Guinea-Bissau?\"""", ""target"": ""\""What is the official name for Guinea-Bissau?\"""", ""references"": ""[\""What is the official name for Guinea-Bissau?\""]..."", ""answers.text"": ""[\""the Republic of Guinea-Bissau\""]"", ""answers.answer_start"": ""[59]""}.

 




[8] the-coorporation/the_squad_qg
: Description-A preprocessed version of the Standford Question Answering Dataset (SQuAD) version 2.0 consisting of contexts and questions only.

Duplicate contexts have been removed and corresponding questions have been merged into an array per context.

Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. 
SQuAD 2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] v2
: The columns in this config are context, questions.
 An example row from this config is {""context"": ""\""Australia: The event was held in Canberra, Austra..."", ""questions"": ""\""When did the torch arrive in Canberra? {sep_token...""}.

 

	[c] v1
: The columns in this config are context, questions.
 An example row from this config is {""context"": ""\""Australia: The event was held in Canberra, Austra..."", ""questions"": ""\""When did the torch arrive in Canberra? {sep_token...""}.

 




[9] gexai/inquisitiveqg
: Description-A dataset of about 20k questions that are elicited from readers as they naturally read through a document sentence by sentence. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. Because these questions are generated while the readers are processing the information, the questions directly communicate gaps between the reader’s and writer’s knowledge about the events described in the text, and are not necessarily answered in the document itself. This type of question reflects a real-world scenario: if one has questions during reading, some of them are answered by the text later on, the rest are not, but any of them would help further the reader’s understanding at the particular point when they asked it. This resource could enable question generation models to simulate human-like curiosity and cognitive processing, which may open up a new realm of applications.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, article_id, article, sentence_id, sentence, span, question, span_start_position, span_end_position.
 An example row from this config is {""id"": ""0"", ""article_id"": ""151"", ""article"": ""\""1 Japan's Daiwa Securities Co. named Masahiro Doz..."", ""sentence_id"": ""1"", ""sentence"": ""\""Japan ' s Daiwa Securities Co . named Masahiro Do..."", ""span"": ""\""named Masahiro Dozen president\"""", ""question"": ""\""Why has Dozen chosen to be president?\"""", ""span_start_position"": ""7"", ""span_end_position"": ""11""}.

 




[10] trivia_qa
: Description-TriviaqQA is a reading comprehension dataset containing over 650K
question-answer-evidence triples. TriviaqQA includes 95K question-answer
pairs authored by trivia enthusiasts and independently gathered evidence
documents, six per question on average, that provide high quality distant
supervision for answering the questions.
.
. This dataset has the following configs:
  

	[b] rc.web.nocontext
: The columns in this config are question, question_id, question_source, entity_pages_doc_source, entity_pages_filename, entity_pages_title, entity_pages_wiki_context, search_results_description, search_results_filename, search_results_rank, search_results_title, search_results_url, search_results_search_context, answer_aliases, answer_normalized_aliases, answer_matched_wiki_entity_name, answer_normalized_matched_wiki_entity_name, answer_normalized_value, answer_type, answer_value.
 An example row from this config is {""question"": ""\""Which American-born Sinclair won the Nobel Prize ..."", ""question_id"": ""\""tc_1\"""", ""question_source"": ""\""http://www.triviacountry.com/\"""", ""entity_pages.doc_source"": ""[]"", ""entity_pages.filename"": ""[]"", ""entity_pages.title"": ""[]"", ""entity_pages.wiki_context"": ""[]"", ""search_results.description"": ""[]"", ""search_results.filename"": ""[]"", ""search_results.rank"": ""[]"", ""search_results.title"": ""[]"", ""search_results.url"": ""[]"", ""search_results.search_context"": ""[]"", ""answer.aliases"": ""[\""(Harry) Sinclair Lewis\"", \""Harry Sinclair Lewis\"",..."", ""answer.normalized_aliases"": ""[\""grace hegger\"", \""lewis harry sinclair\"", \""harry si..."", ""answer.matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_value"": ""\""sinclair lewis\"""", ""answer.type"": ""\""WikipediaEntity\"""", ""answer.value"": ""\""Sinclair Lewis\""""}.

 

	[c] rc.wikipedia.nocontext
: The columns in this config are question, question_id, question_source, entity_pages_doc_source, entity_pages_filename, entity_pages_title, entity_pages_wiki_context, search_results_description, search_results_filename, search_results_rank, search_results_title, search_results_url, search_results_search_context, answer_aliases, answer_normalized_aliases, answer_matched_wiki_entity_name, answer_normalized_matched_wiki_entity_name, answer_normalized_value, answer_type, answer_value.
 An example row from this config is {""question"": ""\""Where in England was Dame Judi Dench born?\"""", ""question_id"": ""\""tc_3\"""", ""question_source"": ""\""http://www.triviacountry.com/\"""", ""entity_pages.doc_source"": ""[]"", ""entity_pages.filename"": ""[]"", ""entity_pages.title"": ""[]"", ""entity_pages.wiki_context"": ""[]"", ""search_results.description"": ""[]"", ""search_results.filename"": ""[]"", ""search_results.rank"": ""[]"", ""search_results.title"": ""[]"", ""search_results.url"": ""[]"", ""search_results.search_context"": ""[]"", ""answer.aliases"": ""[\""Park Grove (1895)\"", \""York UA\"", \""Yorkish\"", \""UN/LO..."", ""answer.normalized_aliases"": ""[\""york yorkshire\"", \""eoferwic\"", \""park grove primary..."", ""answer.matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_value"": ""\""york\"""", ""answer.type"": ""\""WikipediaEntity\"""", ""answer.value"": ""\""York\""""}.

 

	[d] rc.wikipedia
: The columns in this config are question, question_id, question_source, entity_pages_doc_source, entity_pages_filename, entity_pages_title, entity_pages_wiki_context, search_results_description, search_results_filename, search_results_rank, search_results_title, search_results_url, search_results_search_context, answer_aliases, answer_normalized_aliases, answer_matched_wiki_entity_name, answer_normalized_matched_wiki_entity_name, answer_normalized_value, answer_type, answer_value.
 An example row from this config is {""question"": ""\""Where in England was Dame Judi Dench born?\"""", ""question_id"": ""\""tc_3\"""", ""question_source"": ""\""http://www.triviacountry.com/\"""", ""entity_pages.doc_source"": ""[\""TagMe\"", \""TagMe\""]"", ""entity_pages.filename"": ""[\""England.txt\"", \""Judi_Dench.txt\""]"", ""entity_pages.title"": ""[\""England\"", \""Judi Dench\""]"", ""entity_pages.wiki_context"": ""[\""England is a country that is part of the United ..."", ""search_results.description"": ""[]"", ""search_results.filename"": ""[]"", ""search_results.rank"": ""[]"", ""search_results.title"": ""[]"", ""search_results.url"": ""[]"", ""search_results.search_context"": ""[]"", ""answer.aliases"": ""[\""Park Grove (1895)\"", \""York UA\"", \""Yorkish\"", \""UN/LO..."", ""answer.normalized_aliases"": ""[\""york yorkshire\"", \""eoferwic\"", \""park grove primary..."", ""answer.matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_value"": ""\""york\"""", ""answer.type"": ""\""WikipediaEntity\"""", ""answer.value"": ""\""York\""""}.

 




[11] dyk
: Description-The Did You Know (pol. Czy wiesz?) dataset consists of human-annotated question-answer pairs. The task is to predict if the answer is correct. We chose the negatives which have the largest token overlap with a question.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are q_id, question, answer, target.
 An example row from this config is {""q_id"": ""\""czywiesz4068\"""", ""question"": ""\""z jakiego powodu zwo\\u0142ano synod w Whitby?\"""", ""answer"": ""\""W\\u015br\\u00f3d mnich\\u00f3w i mniszek mieszkaj\\u..."", ""target"": ""0""}.

 




[12] daily_dialog
: Description-We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects.
The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way
and cover various topics about our daily life. We also manually label the developed dataset with communication
intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it
benefit the research field of dialog systems.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are dialog, act, emotion.
 An example row from this config is {""dialog"": ""[\""Say , Jim , how about going for a few beers afte..."", ""act"": ""[3, 4, 2, 2, 2, 3, 4, 1, 3, 4]"", ""emotion"": ""[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]""}.

 




[13] pn_summary
: Description-A well-structured summarization dataset for the Persian language consists of 93,207 records. It is prepared for Abstractive/Extractive tasks (like cnn_dailymail for English). It can also be used in other scopes like Text Generation, Title Generation, and News Category Classification.
It is imperative to consider that the newlines were replaced with the `[n]` symbol. Please interpret them into normal newlines (for ex. `t.replace(""[n]"", ""
"")`) and then use them for your purposes.
.
. This dataset has the following configs:
  

	[b] 1.0.0
: The columns in this config are id, title, article, summary, category, categories, network, link.
 An example row from this config is {""id"": ""\""738e296491f8b24c5aa63e9829fd249fb4428a66\"""", ""title"": ""\""\\u0645\\u062f\\u06cc\\u0631\\u06cc\\u062a \\u0641\\u0631..."", ""article"": ""\""\\u0628\\u0647 \\u06af\\u0632\\u0627\\u0631\\u0634 \\u063..."", ""summary"": ""\""\\u0645\\u062f\\u06cc\\u0631\\u0639\\u0627\\u0645\\u0644 ..."", ""category"": ""5"", ""categories"": ""\""\\u0646\\u0641\\u062a\"""", ""network"": ""2"", ""link"": ""\""https://www.shana.ir/news/275284/%D9%85%D8%AF%DB%...""}.

 





The reranking results of the 13 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,squad_v2,squad_v2
task033:winogrande answer generation,"You need to answer a given question containing a blank (_). Your answer must be one of the two objects mentioned in the question, for example \""trophy\"" and \""suitcase\"". Your answer must not contain a word that is not present in the question. Please don't use articles (e.g., the, a) before the answer.""
{
            ""input"": ""The trophy doesn't fit into the brown suitcase because _ is too large."",
            ""output"": ""trophy"",
            ""explanation"": ""Answer is one of the objects (\""trophy\"" and \""suitcase\"") in the question. Since the blank is a \""large\"" object that didn't fit the \""suitcase\"", the answer must be \""trophy\"".""
        },
        {
            ""input"": ""Grace was happy to trade me her sweater for my jacket. She thinks _ looks dowdy on her."",
            ""output"": ""sweater"",
            ""explanation"": ""The word \""dowdy\"" decides the answer among the objects (\""sweater\"" and \""jacket\"") present in the question.""
        },
        {
            ""input"": ""While redecorating her home, Sam took out the carpet and replaced it with wood floors. The _ was old."",
            ""output"": ""carpet"",
            ""explanation"": ""The blank is \""old\"", it must be what gets \""replaced\"", which has to be \""carpet\"".""
        },",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 You need to answer a given question containing a blank (_). Your answer must be one of the two objects mentioned in the question, for example ""trophy"" and ""suitcase"". Your answer must not contain a word that is not present in the question. Please don't use articles (e.g., the, a) before the answer. and these are some examples of the same: {
            ""input"": ""The trophy doesn't fit into the brown suitcase because _ is too large."",
            ""output"": ""trophy"",
            ""explanation"": ""Answer is one of the objects (\""trophy\"" and \""suitcase\"") in the question. Since the blank is a \""large\"" object that didn't fit the \""suitcase\"", the answer must be \""trophy\"".""
        },
        {
            ""input"": ""Grace was happy to trade me her sweater for my jacket. She thinks _ looks dowdy on her."",
            ""output"": ""sweater"",
            ""explanation"": ""The word \""dowdy\"" decides the answer among the objects (\""sweater\"" and \""jacket\"") present in the question.""
        },
        {
            ""input"": ""While redecorating her home, Sam took out the carpet and replaced it with wood floors. The _ was old."",
            ""output"": ""carpet"",
            ""explanation"": ""The blank is \""old\"", it must be what gets \""replaced\"", which has to be \""carpet\"".""
        } 

There are 14 datasets available for this task, each indicated by number identifier []. 

[1] dyk
: Description-The Did You Know (pol. Czy wiesz?) dataset consists of human-annotated question-answer pairs. The task is to predict if the answer is correct. We chose the negatives which have the largest token overlap with a question.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are q_id, question, answer, target.
 An example row from this config is {""q_id"": ""\""czywiesz4068\"""", ""question"": ""\""z jakiego powodu zwo\\u0142ano synod w Whitby?\"""", ""answer"": ""\""W\\u015br\\u00f3d mnich\\u00f3w i mniszek mieszkaj\\u..."", ""target"": ""0""}.

 




[2] SajjadAyoubi/persian_qa
: Description-\\\\
Persian Question Answering (PersianQA) Dataset is a reading comprehension dataset on Persian Wikipedia. 
The crowd-sourced dataset consists of more than 9,000 entries. Each entry can be either an impossible to answer or a question with one or more answers spanning in the passage (the context) from which the questioner proposed the question. Much like the SQuAD2.0 dataset, the impossible or unanswerable questions can be utilized to create a system which ""knows that it doesn't know the answer"".
.
. This dataset has the following configs:
  

	[b] persian_qa
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""1"", ""title"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""context"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""question"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""answers.text"": ""[\""\\u062f\\u0631 \\u0634\\u0631\\u0642 \\u0634\\u0647\\u06..."", ""answers.answer_start"": ""[114]""}.

 




[3] trivia_qa
: Description-TriviaqQA is a reading comprehension dataset containing over 650K
question-answer-evidence triples. TriviaqQA includes 95K question-answer
pairs authored by trivia enthusiasts and independently gathered evidence
documents, six per question on average, that provide high quality distant
supervision for answering the questions.
.
. This dataset has the following configs:
  

	[b] rc.wikipedia.nocontext
: The columns in this config are question, question_id, question_source, entity_pages_doc_source, entity_pages_filename, entity_pages_title, entity_pages_wiki_context, search_results_description, search_results_filename, search_results_rank, search_results_title, search_results_url, search_results_search_context, answer_aliases, answer_normalized_aliases, answer_matched_wiki_entity_name, answer_normalized_matched_wiki_entity_name, answer_normalized_value, answer_type, answer_value.
 An example row from this config is {""question"": ""\""Where in England was Dame Judi Dench born?\"""", ""question_id"": ""\""tc_3\"""", ""question_source"": ""\""http://www.triviacountry.com/\"""", ""entity_pages.doc_source"": ""[]"", ""entity_pages.filename"": ""[]"", ""entity_pages.title"": ""[]"", ""entity_pages.wiki_context"": ""[]"", ""search_results.description"": ""[]"", ""search_results.filename"": ""[]"", ""search_results.rank"": ""[]"", ""search_results.title"": ""[]"", ""search_results.url"": ""[]"", ""search_results.search_context"": ""[]"", ""answer.aliases"": ""[\""Park Grove (1895)\"", \""York UA\"", \""Yorkish\"", \""UN/LO..."", ""answer.normalized_aliases"": ""[\""york yorkshire\"", \""eoferwic\"", \""park grove primary..."", ""answer.matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_value"": ""\""york\"""", ""answer.type"": ""\""WikipediaEntity\"""", ""answer.value"": ""\""York\""""}.

 

	[c] unfiltered.nocontext
: The columns in this config are question, question_id, question_source, entity_pages_doc_source, entity_pages_filename, entity_pages_title, entity_pages_wiki_context, search_results_description, search_results_filename, search_results_rank, search_results_title, search_results_url, search_results_search_context, answer_aliases, answer_normalized_aliases, answer_matched_wiki_entity_name, answer_normalized_matched_wiki_entity_name, answer_normalized_value, answer_type, answer_value.
 An example row from this config is {""question"": ""\""Who was President when the first Peanuts cartoon ..."", ""question_id"": ""\""tc_0\"""", ""question_source"": ""\""http://www.triviacountry.com/\"""", ""entity_pages.doc_source"": ""[]"", ""entity_pages.filename"": ""[]"", ""entity_pages.title"": ""[]"", ""entity_pages.wiki_context"": ""[]"", ""search_results.description"": ""[]"", ""search_results.filename"": ""[]"", ""search_results.rank"": ""[]"", ""search_results.title"": ""[]"", ""search_results.url"": ""[]"", ""search_results.search_context"": ""[]"", ""answer.aliases"": ""[\""Presidency of Harry S. Truman\"", \""Hary truman\"", \""..."", ""answer.normalized_aliases"": ""[\""presidency of harry s truman\"", \""33rd president o..."", ""answer.matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_matched_wiki_entity_name"": ""\""\"""", ""answer.normalized_value"": ""\""harry truman\"""", ""answer.type"": ""\""WikipediaEntity\"""", ""answer.value"": ""\""Harry Truman\""""}.

 




[4] bookcorpus
: Description-Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story.This work aims to align books to their movie releases in order to providerich descriptive explanations for visual content that go semantically farbeyond the captions available in current datasets. .
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are text.
 An example row from this config is {""text"": ""\""usually , he would be tearing around the living r...""}.

 




[5] dbpedia_14
: Description-The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes
from DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we
randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size
of the training dataset is 560,000 and testing dataset 70,000.
There are 3 columns in the dataset (same for train and test splits), corresponding to class index
(1 to 14), title and content. The title and content are escaped using double quotes (""), and any
internal double quote is escaped by 2 double quotes (""""). There are no new lines in title or content.
.
. This dataset has the following configs:
  

	[b] dbpedia_14
: The columns in this config are label, title, content.
 An example row from this config is {""label"": ""0"", ""title"": ""\""E. D. Abbott Ltd\"""", ""content"": ""\"" Abbott of Farnham E D Abbott Limited was a Briti...""}.

 




[6] the-coorporation/the_squad_qg
: Description-A preprocessed version of the Standford Question Answering Dataset (SQuAD) version 2.0 consisting of contexts and questions only.

Duplicate contexts have been removed and corresponding questions have been merged into an array per context.

Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. 
SQuAD 2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] v2
: The columns in this config are context, questions.
 An example row from this config is {""context"": ""\""Australia: The event was held in Canberra, Austra..."", ""questions"": ""\""When did the torch arrive in Canberra? {sep_token...""}.

 

	[c] v1
: The columns in this config are context, questions.
 An example row from this config is {""context"": ""\""Australia: The event was held in Canberra, Austra..."", ""questions"": ""\""When did the torch arrive in Canberra? {sep_token...""}.

 




[7] daily_dialog
: Description-We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects.
The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way
and cover various topics about our daily life. We also manually label the developed dataset with communication
intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it
benefit the research field of dialog systems.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are dialog, act, emotion.
 An example row from this config is {""dialog"": ""[\""Say , Jim , how about going for a few beers afte..."", ""act"": ""[3, 4, 2, 2, 2, 3, 4, 1, 3, 4]"", ""emotion"": ""[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]""}.

 




[8] pn_summary
: Description-A well-structured summarization dataset for the Persian language consists of 93,207 records. It is prepared for Abstractive/Extractive tasks (like cnn_dailymail for English). It can also be used in other scopes like Text Generation, Title Generation, and News Category Classification.
It is imperative to consider that the newlines were replaced with the `[n]` symbol. Please interpret them into normal newlines (for ex. `t.replace(""[n]"", ""
"")`) and then use them for your purposes.
.
. This dataset has the following configs:
  

	[b] 1.0.0
: The columns in this config are id, title, article, summary, category, categories, network, link.
 An example row from this config is {""id"": ""\""738e296491f8b24c5aa63e9829fd249fb4428a66\"""", ""title"": ""\""\\u0645\\u062f\\u06cc\\u0631\\u06cc\\u062a \\u0641\\u0631..."", ""article"": ""\""\\u0628\\u0647 \\u06af\\u0632\\u0627\\u0631\\u0634 \\u063..."", ""summary"": ""\""\\u0645\\u062f\\u06cc\\u0631\\u0639\\u0627\\u0645\\u0644 ..."", ""category"": ""5"", ""categories"": ""\""\\u0646\\u0641\\u062a\"""", ""network"": ""2"", ""link"": ""\""https://www.shana.ir/news/275284/%D9%85%D8%AF%DB%...""}.

 




[9] yulongmannlp/dev_para
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[10] yulongmannlp/dev_orig
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[11] yulongmannlp/adv_para
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[12] yulongmannlp/adv_ori
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[13] squad
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[14] lhoestq/squad
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 





The reranking results of the 14 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,,
task073_commonsenseqa_answer_generation," ""You are given a question and some answer options (associated with \""A\"", \""B\"", \""C\"", \""D\""). You should choose the correct answer based on commonsense knowledge. Avoid answering questions based on associations, the set of answers are chosen deliberately to capture common sense beyond associations. Do not generate anything else apart from one of the following characters: 'A', 'B, 'C', 'D', 'E' and only give one answer for each question.""
  {
            ""input"": ""Where would you find magazines along side many other printed works?\n(A)doctor (B)bookstore (C)market (D)train station (E)mortuary"",
            ""output"": ""B"",
            ""explanation"": ""libraries contains magazines and many other printed works.""
        },
        {
            ""input"": ""What island country is ferret popular?\n(A)own home (B)north carolina (C)great britain (D)hutch (E)outdoors"",
            ""output"": ""C"",
            ""explanation"": ""great britain is the only island country in the choices.""
        }",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 You are given a question and some answer options (associated with ""A"", ""B"", ""C"", ""D""). You should choose the correct answer based on commonsense knowledge. Avoid answering questions based on associations, the set of answers are chosen deliberately to capture common sense beyond associations. Do not generate anything else apart from one of the following characters: 'A', 'B, 'C', 'D', 'E' and only give one answer for each question. and these are some examples of the same: {
            ""input"": ""Where would you find magazines along side many other printed works?\n(A)doctor (B)bookstore (C)market (D)train station (E)mortuary"",
            ""output"": ""B"",
            ""explanation"": ""libraries contains magazines and many other printed works.""
        },
        {
            ""input"": ""What island country is ferret popular?\n(A)own home (B)north carolina (C)great britain (D)hutch (E)outdoors"",
            ""output"": ""C"",
            ""explanation"": ""great britain is the only island country in the choices.""
        } 

There are 15 datasets available for this task, each indicated by number identifier []. 

[1] squad_v2
: Description-combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers
 to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but
 also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] squad_v2
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""56be85543aeaaa14008c9063\"""", ""title"": ""\""Beyonc\\u00e9\"""", ""context"": ""\""Beyonc\\u00e9 Giselle Knowles-Carter (/bi\\u02d0\\u0..."", ""question"": ""\""When did Beyonce start becoming popular?\"""", ""answers.text"": ""[\""in the late 1990s\""]"", ""answers.answer_start"": ""[269]""}.

 




[2] shivmoha/squad-unanswerable
: Description-combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers
 to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but
 also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] squad_v2
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5a8d7bf7df8bba001a0f9ab1\"""", ""title"": ""\""The_Legend_of_Zelda:_Twilight_Princess\"""", ""context"": ""\""The Legend of Zelda: Twilight Princess (Japanese:..."", ""question"": ""\""What category of game is Legend of Zelda: Austral..."", ""answers.text"": ""[]"", ""answers.answer_start"": ""[]""}.

 




[3] TurkuNLP/squad_v2_fi
: Description-combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers
 to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but
 also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] squad_v2_fi
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""56be85543aeaaa14008c9063\"""", ""title"": ""\""Beyonc\\u00e9\"""", ""context"": ""\""Beyonc\\u00e9 Giselle Knowles-Carter (/bi\\u02d0\\u0..."", ""question"": ""\""Milloin Beyoncesta alkoi tulla suosittu?\"""", ""answers.text"": ""[\""1990-luvun lopulla\""]"", ""answers.answer_start"": ""[274]""}.

 




[4] GEM/squad_v2
: Description- SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers
 to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but
 also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] gem_data_split
: The columns in this config are gem_id, id, title, context, question, target, references, answers_text, answers_answer_start.
 An example row from this config is {""gem_id"": ""\""gem-squad_v2-train-0\"""", ""id"": ""\""56f89ee99b226e1400dd0cd5\"""", ""title"": ""\""Guinea-Bissau\"""", ""context"": ""\""Guinea-Bissau (i/\\u02c8\\u0261\\u026ani b\\u026a\\u02..."", ""question"": ""\""What is the official name for Guinea-Bissau?\"""", ""target"": ""\""What is the official name for Guinea-Bissau?\"""", ""references"": ""[\""What is the official name for Guinea-Bissau?\""]..."", ""answers.text"": ""[\""the Republic of Guinea-Bissau\""]"", ""answers.answer_start"": ""[59]""}.

 




[5] conceptnet5
: Description-This dataset is designed to provide training data
for common sense relationships pulls together from various sources.

The dataset is multi-lingual. See langauge codes and language info
here: https://github.com/commonsense/conceptnet5/wiki/Languages


This dataset provides an interface for the conceptnet5 csv file, and
some (but not all) of the raw text data used to build conceptnet5:
omcsnet_sentences_free.txt, and omcsnet_sentences_more.txt.

One use of this dataset would be to learn to extract the conceptnet
relationship from the omcsnet sentences.

Conceptnet5 has 34,074,917 relationships. Of those relationships,
there are 2,176,099 surface text sentences related to those 2M
entries.

omcsnet_sentences_free has 898,161 lines. omcsnet_sentences_more has
2,001,736 lines.

Original downloads are available here
https://github.com/commonsense/conceptnet5/wiki/Downloads. For more
information, see: https://github.com/commonsense/conceptnet5/wiki

The omcsnet data comes with the following warning from the authors of
the above site: Remember: this data comes from various forms of
crowdsourcing. Sentences in these files are not necessarily true,
useful, or appropriate.

.
. This dataset has the following configs:
  

	[b] conceptnet5
: The columns in this config are sentence, full_rel, rel, arg1, arg2, lang, extra_info, weight.
 An example row from this config is {""sentence"": ""\""\"""", ""full_rel"": ""\""/a/[/r/Antonym/,/c/ab/\\u0430\\u0433\\u044b\\u0440\\u0..."", ""rel"": ""\""/r/Antonym\"""", ""arg1"": ""\""/c/ab/\\u0430\\u0433\\u044b\\u0440\\u0443\\u0430/n\"""", ""arg2"": ""\""/c/ab/\\u0430\\u04a7\\u0441\\u0443\\u0430\"""", ""lang"": ""\""ab\"""", ""extra_info"": ""\""{\\\""dataset\\\"": \\\""/d/wiktionary/en\\\"", \\\""license\\\"": ..."", ""weight"": ""1.0""}.

 

	[c] omcs_sentences_free
: The columns in this config are sentence, raw_data, lang.
 An example row from this config is {""sentence"": ""\""text\"""", ""raw_data"": ""\""id\\ttext\\tcreator_id\\tcreated_on\\tlanguage_id\\tac..."", ""lang"": ""\""language_id\""""}.

 

	[d] omcs_sentences_more
: The columns in this config are sentence, raw_data, lang.
 An example row from this config is {""sentence"": ""\""text\"""", ""raw_data"": ""\""id\\ttext\\tcreator_id\\tcreated_on\\tlanguage_id\\tac..."", ""lang"": ""\""language_id\""""}.

 




[6] dbpedia_14
: Description-The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes
from DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we
randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size
of the training dataset is 560,000 and testing dataset 70,000.
There are 3 columns in the dataset (same for train and test splits), corresponding to class index
(1 to 14), title and content. The title and content are escaped using double quotes (""), and any
internal double quote is escaped by 2 double quotes (""""). There are no new lines in title or content.
.
. This dataset has the following configs:
  

	[b] dbpedia_14
: The columns in this config are label, title, content.
 An example row from this config is {""label"": ""0"", ""title"": ""\""E. D. Abbott Ltd\"""", ""content"": ""\"" Abbott of Farnham E D Abbott Limited was a Briti...""}.

 




[7] tydiqa
: Description-TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.
The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language
expresses -- such that we expect models performing well on this set to generalize across a large number of the languages
in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic
information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but
don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without
the use of translation (unlike MLQA and XQuAD).
.
. This dataset has the following configs:
  

	[b] primary_task
: The columns in this config are passage_answer_candidates_plaintext_start_byte, passage_answer_candidates_plaintext_end_byte, question_text, document_title, language, annotations_passage_answer_candidate_index, annotations_minimal_answers_start_byte, annotations_minimal_answers_end_byte, annotations_yes_no_answer, document_plaintext, document_url.
 An example row from this config is {""passage_answer_candidates.plaintext_start_byte"": ""[1, 660, 844, 1196, 1354, 2126, 3608, 4198, 4509, ..."", ""passage_answer_candidates.plaintext_end_byte"": ""[659, 843, 1195, 1353, 2125, 3607, 4161, 4508, 474..."", ""question_text"": ""\""berapakah jenis ras yang ada didunia?\"""", ""document_title"": ""\""Ras manusia\"""", ""language"": ""\""indonesian\"""", ""annotations.passage_answer_candidate_index"": ""[-1]"", ""annotations.minimal_answers_start_byte"": ""[-1]"", ""annotations.minimal_answers_end_byte"": ""[-1]"", ""annotations.yes_no_answer"": ""[\""NONE\""]"", ""document_plaintext"": ""\""\\ntransl.\\n\\nRas (dari bahasa Prancis race, yang ..."", ""document_url"": ""\""https://id.wikipedia.org/wiki/Ras%20manusia\""""}.

 

	[c] secondary_task
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""finnish--7633091408814529542-0\"""", ""title"": ""\""Charles Fort\"""", ""context"": ""\""Charles Hoy Fort (6. elokuuta (joidenkin l\\u00e4h..."", ""question"": ""\""Milloin Charles Fort syntyi?\"""", ""answers.text"": ""[\""6. elokuuta (joidenkin l\\u00e4hteiden mukaan 9.)..."", ""answers.answer_start"": ""[18]""}.

 




[8] khalidalt/tydiqa-goldp
: Description-TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.
The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language
expresses -- such that we expect models performing well on this set to generalize across a large number of the languages
in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic
information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but
don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without
the use of translation (unlike MLQA and XQuAD).
.
. This dataset has the following configs:
  

	[b] japanese
: The columns in this config are id, language, document_title, passage_text, question_text, answers_text, answers_start_byte, answers_limit_byte.
 An example row from this config is {""id"": ""\""4547119098444904715-1\"""", ""language"": ""\""japanese\"""", ""document_title"": ""\""\\u30c0\\u30cb\\u30a8\\u30eb\\u30fbJ\\u30fb\\u30ad\\u30e3..."", ""passage_text"": ""\""\\u201c\\u30c0\\u30f3\\u201d\\u3053\\u3068\\u3001\\u30c0\\..."", ""question_text"": ""\""\\u201c\\u30c0\\u30f3\\u201d \\u30c0\\u30cb\\u30a8\\u30eb..."", ""answers.text"": ""[\""\\u30ab\\u30ea\\u30d5\\u30a9\\u30eb\\u30cb\\u30a2\\u5dde..."", ""answers.start_byte"": ""[91]"", ""answers.limit_byte"": ""[139]""}.

 

	[c] english
: The columns in this config are id, language, document_title, passage_text, question_text, answers_text, answers_start_byte, answers_limit_byte.
 An example row from this config is {""id"": ""\""-9041575374418655524-12\"""", ""language"": ""\""english\"""", ""document_title"": ""\""Quantum field theory\"""", ""passage_text"": ""\""Quantum field theory naturally began with the stu..."", ""question_text"": ""\""When was quantum field theory developed?\"""", ""answers.text"": ""[\""1920s\""]"", ""answers.start_byte"": ""[159]"", ""answers.limit_byte"": ""[164]""}.

 

	[d] swahili
: The columns in this config are id, language, document_title, passage_text, question_text, answers_text, answers_start_byte, answers_limit_byte.
 An example row from this config is {""id"": ""\""-2755285734550319079-12\"""", ""language"": ""\""swahili\"""", ""document_title"": ""\""Chungwa\"""", ""passage_text"": ""\""Machungwa kwa kawaida hulimwa kwaajili ya biashar..."", ""question_text"": ""\""Je,nchi gani yenye kuzalisha chungwa kwa wingi za..."", ""answers.text"": ""[\""Brazili, Marekani na Meksiko\""]"", ""answers.start_byte"": ""[132]"", ""answers.limit_byte"": ""[160]""}.

 

	[e] indonesian
: The columns in this config are id, language, document_title, passage_text, question_text, answers_text, answers_start_byte, answers_limit_byte.
 An example row from this config is {""id"": ""\""496955121539262633-33\"""", ""language"": ""\""indonesian\"""", ""document_title"": ""\""Ernest Douwes Dekker\"""", ""passage_text"": ""\""Ernest Douwes Dekker wafat dini hari tanggal 28 A..."", ""question_text"": ""\""dimanakah  Dr. Ernest Fran\\u00e7ois Eug\\u00e8ne D..."", ""answers.text"": ""[\""28 Agustus 1950\""]"", ""answers.start_byte"": ""[45]"", ""answers.limit_byte"": ""[60]""}.

 

	[f] finnish
: The columns in this config are id, language, document_title, passage_text, question_text, answers_text, answers_start_byte, answers_limit_byte.
 An example row from this config is {""id"": ""\""-7633091408814529542-0\"""", ""language"": ""\""finnish\"""", ""document_title"": ""\""Charles Fort\"""", ""passage_text"": ""\""Charles Hoy Fort (6. elokuuta (joidenkin l\\u00e4h..."", ""question_text"": ""\""Milloin Charles Fort syntyi?\"""", ""answers.text"": ""[\""6. elokuuta (joidenkin l\\u00e4hteiden mukaan 9.)..."", ""answers.start_byte"": ""[18]"", ""answers.limit_byte"": ""[67]""}.

 




[9] juletxara/tydiqa_xtreme
: Description-TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.
The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language
expresses -- such that we expect models performing well on this set to generalize across a large number of the languages
in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic
information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but
don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without
the use of translation (unlike MLQA and XQuAD).

We also include ""translate-train"" and ""translate-test"" splits for each non-English languages from XTREME (Hu et al., 2020). These splits are the automatic translations from English to each target language used in the XTREME paper [https://arxiv.org/abs/2003.11080]. The ""translate-train"" split purposefully ignores the non-English TyDiQA-GoldP training data to simulate the transfer learning scenario where original-language data is not available and system builders must rely on labeled English data plus existing machine translation systems.
.
. This dataset has the following configs:
  

	[b] ru
: The columns in this config are id, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""0\"""", ""context"": ""\""\\u041a\\u0430\\u0440\\u0438\\u0301\\u0431\\u0441\\u043a\\..."", ""question"": ""\""\\u041a\\u043e\\u0433\\u0434\\u0430 \\u043d\\u0430\\u0447..."", ""answers.text"": ""[\""\\u043e\\u043a\\u0442\\u044f\\u0431\\u0440\\u0435 1962\""..."", ""answers.answer_start"": ""[347]""}.

 

	[c] ar
: The columns in this config are id, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""0\"""", ""context"": ""\""\\u0627\\u0644\\u0645\\u0633\\u0623\\u0644\\u0629 \\u0627..."", ""question"": ""\""\\u0645\\u0627 \\u0647\\u064a \\u0627\\u0644\\u0645\\u063..."", ""answers.text"": ""[\""\\u0645\\u0633\\u0623\\u0644\\u0629 \\u0648\\u062c\\u064..."", ""answers.answer_start"": ""[124]""}.

 

	[d] fi
: The columns in this config are id, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""0\"""", ""context"": ""\""Charles Hoy Fort (6. elokuuta (joidenkin l\\u00e4h..."", ""question"": ""\""Milloin Charles Fort syntyi?\"""", ""answers.text"": ""[\""6. elokuuta (joidenkin l\\u00e4hteiden mukaan 9.)..."", ""answers.answer_start"": ""[18]""}.

 

	[e] bn
: The columns in this config are id, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""0\"""", ""context"": ""\""\\u099a\\u09c7\\u0999\\u09cd\\u0997\\u09bf\\u099c \\u0996..."", ""question"": ""\""\\u099a\\u09c7\\u0999\\u09cd\\u0997\\u09bf\\u09b8 \\u0996..."", ""answers.text"": ""[\""\\u09ac\\u09cb\\u09b0\\u099c\\u09bf\\u0997\\u09bf\\u09a8..."", ""answers.answer_start"": ""[1028]""}.

 




[10] commonsense_qa
: Description-CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge
to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.
The dataset is provided in two major training/validation/testing set splits: ""Random split"" which is the main evaluation
split, and ""Question token split"", see paper for details.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are id, question, question_concept, choices_label, choices_text, answerKey.
 An example row from this config is {""id"": ""\""075e483d21c29a511267ef62bedc0461\"""", ""question"": ""\""The sanctions against the school were a punishing..."", ""question_concept"": ""\""punishing\"""", ""choices.label"": ""[\""A\"", \""B\"", \""C\"", \""D\"", \""E\""]"", ""choices.text"": ""[\""ignore\"", \""enforce\"", \""authoritarian\"", \""yell at\"", ..."", ""answerKey"": ""\""A\""""}.

 




[11] gexai/inquisitiveqg
: Description-A dataset of about 20k questions that are elicited from readers as they naturally read through a document sentence by sentence. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. Because these questions are generated while the readers are processing the information, the questions directly communicate gaps between the reader’s and writer’s knowledge about the events described in the text, and are not necessarily answered in the document itself. This type of question reflects a real-world scenario: if one has questions during reading, some of them are answered by the text later on, the rest are not, but any of them would help further the reader’s understanding at the particular point when they asked it. This resource could enable question generation models to simulate human-like curiosity and cognitive processing, which may open up a new realm of applications.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, article_id, article, sentence_id, sentence, span, question, span_start_position, span_end_position.
 An example row from this config is {""id"": ""0"", ""article_id"": ""151"", ""article"": ""\""1 Japan's Daiwa Securities Co. named Masahiro Doz..."", ""sentence_id"": ""1"", ""sentence"": ""\""Japan ' s Daiwa Securities Co . named Masahiro Do..."", ""span"": ""\""named Masahiro Dozen president\"""", ""question"": ""\""Why has Dozen chosen to be president?\"""", ""span_start_position"": ""7"", ""span_end_position"": ""11""}.

 




[12] SajjadAyoubi/persian_qa
: Description-\\\\
Persian Question Answering (PersianQA) Dataset is a reading comprehension dataset on Persian Wikipedia. 
The crowd-sourced dataset consists of more than 9,000 entries. Each entry can be either an impossible to answer or a question with one or more answers spanning in the passage (the context) from which the questioner proposed the question. Much like the SQuAD2.0 dataset, the impossible or unanswerable questions can be utilized to create a system which ""knows that it doesn't know the answer"".
.
. This dataset has the following configs:
  

	[b] persian_qa
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""1"", ""title"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""context"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""question"": ""\""\\u0634\\u0631\\u06a9\\u062a \\u0641\\u0648\\u0644\\u0627..."", ""answers.text"": ""[\""\\u062f\\u0631 \\u0634\\u0631\\u0642 \\u0634\\u0647\\u06..."", ""answers.answer_start"": ""[114]""}.

 




[13] imvladikon/parashoot
: Description-
A Hebrew question and answering dataset in the style of SQuAD, based on articles scraped from Wikipedia. The dataset contains a few thousand crowdsource-annotated pairs of questions and answers, in a setting suitable for few-shot learning.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""c370661d-53de-4862-a363-ffc9606fe154\"""", ""title"": ""\""\\u05d0\\u05d4\\u05e8\\u05d5\\u05df \\u05e8\\u05d5\\u05d6..."", ""context"": ""\""\\u05d1\\u05ea\\u05d7\\u05d9\\u05dc\\u05d4 \\u05e2\\u05e1..."", ""question"": ""\""\\u05d1\\u05de\\u05d4 \\u05d1\\u05ea\\u05d7\\u05d9\\u05dc..."", ""answers.text"": ""[\""\\u05de\\u05db\\u05dc \\u05d4\\u05d1\\u05d0 \\u05dc\\u05..."", ""answers.answer_start"": ""[18]""}.

 




[14] its5Q/yandex-q
: Description-This is a dataset of questions and answers scraped from Yandex.Q.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are description, question, answer.
 An example row from this config is {""description"": ""\""\"""", ""question"": ""\""\\u041a\\u0430\\u043a \\u0432\\u043e\\u0439\\u0442\\u0438..."", ""answer"": ""\""\\u041d\\u0438\\u043a\\u0430\\u043a \\u043d\\u043e \\u043...""}.

 




[15] the-coorporation/the_squad_qg
: Description-A preprocessed version of the Standford Question Answering Dataset (SQuAD) version 2.0 consisting of contexts and questions only.

Duplicate contexts have been removed and corresponding questions have been merged into an array per context.

Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. 
SQuAD 2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.
.
. This dataset has the following configs:
  

	[b] v2
: The columns in this config are context, questions.
 An example row from this config is {""context"": ""\""Australia: The event was held in Canberra, Austra..."", ""questions"": ""\""When did the torch arrive in Canberra? {sep_token...""}.

 

	[c] v1
: The columns in this config are context, questions.
 An example row from this config is {""context"": ""\""Australia: The event was held in Canberra, Austra..."", ""questions"": ""\""When did the torch arrive in Canberra? {sep_token...""}.

 





The reranking results of the 15 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,,
task288_gigaword_summarization,"""In this task, you are given a text of the article. Your task is to generate a headline (title) for this article."",
        
       {
            ""input"": ""australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed ."",
            ""output"": ""australian current account deficit narrows sharply"",
            ""explanation"": ""This headline is appropriate for the given article text because the focus of this article is Australian current accounts.""
        },
        {
            ""input"": ""at least two people were killed in a suspected bomb attack on a passenger bus in the strife-torn southern philippines on monday , the military said ."",
            ""output"": ""at least two dead in southern philippines blast"",
            ""explanation"": ""This article focuses on the southern Philippines blast; hence the generated title is correct.""
        },
        {
            ""input"": ""four east timorese youths who scaled the french embassy 's fence here thursday , left the embassy on their way to portugal friday ."",
            ""output"": ""UNK latest east timorese asylum seekers leave for portugal"",
            ""explanation"": ""The example is correct, as it correctly summarizes the passage""
        },",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 ""In this task, you are given a text of the article. Your task is to generate a headline (title) for this article."" and these are some examples of the same: Input: ""australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .""
Output: ""australian current account deficit narrows sharply""
Explanation: ""This headline is appropriate for the given article text because the focus of this article is Australian current accounts.""

Input: ""at least two people were killed in a suspected bomb attack on a passenger bus in the strife-torn southern philippines on monday , the military said .""
Output: ""at least two dead in southern philippines blast""
Explanation: ""This article focuses on the southern Philippines blast; hence the generated title is correct.""

Input: ""four east timorese youths who scaled the french embassy 's fence here thursday , left the embassy on their way to portugal friday .""
Output: ""UNK latest east timorese asylum seekers leave for portugal""
Explanation: ""The example is correct, as it correctly summarizes the passage"" 

There are 21 datasets available for this task, each indicated by number identifier []. 

[1] bookcorpus
: Description-Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story.This work aims to align books to their movie releases in order to providerich descriptive explanations for visual content that go semantically farbeyond the captions available in current datasets. .
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are text.
 An example row from this config is {""text"": ""\""usually , he would be tearing around the living r...""}.

 




[2] vesteinn/swe-nerc
: Description-The corpus consists of ca. 150.000 words of text.
.
. This dataset has the following configs:
  

	[b] swe-nerc
: The columns in this config are id, tokens, ner_tags.
 An example row from this config is {""id"": ""\""0\"""", ""tokens"": ""[\""Det\"", \""har\"", \""iaf\"", \""jag\"", \""gjort\"", \""men\"", \""ska\""..."", ""ner_tags"": ""[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...""}.

 




[3] jakartaresearch/news-title-gen
: Description-This dataset is built for generating text for news title.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are title, link, date.
 An example row from this config is {""title"": ""\""Muncul Temuan Baru, Virus Corona Berasal dari Lab..."", ""link"": ""\""https://www.tribunnews.com/topic/virus-corona\"""", ""date"": ""\""2020-02-21\""""}.

 




[4] daily_dialog
: Description-We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects.
The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way
and cover various topics about our daily life. We also manually label the developed dataset with communication
intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it
benefit the research field of dialog systems.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are dialog, act, emotion.
 An example row from this config is {""dialog"": ""[\""Say , Jim , how about going for a few beers afte..."", ""act"": ""[3, 4, 2, 2, 2, 3, 4, 1, 3, 4]"", ""emotion"": ""[0, 0, 0, 0, 0, 0, 4, 4, 4, 4]""}.

 




[5] sst2
: Description-The Stanford Sentiment Treebank consists of sentences from movie reviews and
human annotations of their sentiment. The task is to predict the sentiment of a
given sentence. We use the two-way (positive/negative) class split, and use only
sentence-level labels.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are idx, sentence, label.
 An example row from this config is {""idx"": ""0"", ""sentence"": ""\""hide new secretions from the parental units \"""", ""label"": ""0""}.

 




[6] maritaca-ai/sst2_pt
: Description-The Stanford Sentiment Treebank consists of sentences from movie reviews and
human annotations of their sentiment. The task is to predict the sentiment of a
given sentence. We use the two-way (positive/negative) class split, and use only
sentence-level labels.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""esconder novas secre\\u00e7\\u00f5es das unidades p..."", ""label"": ""0""}.

 




[7] ptb_text_only
: Description-This is the Penn Treebank Project: Release 2 CDROM, featuring a million words of 1989 Wall Street Journal material. This corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure.
.
. This dataset has the following configs:
  

	[b] penn_treebank
: The columns in this config are sentence.
 An example row from this config is {""sentence"": ""\""aer banknote berlitz calloway centrust cluett fro...""}.

 




[8] SocialGrep/the-reddit-place-dataset
: Description-The written history or /r/Place, in posts and comments.
.
. This dataset has the following configs:
  

	[b] posts
: The columns in this config are type, id, subreddit_id, subreddit_name, subreddit_nsfw, created_utc, permalink, domain, url, selftext, title, score.
 An example row from this config is {""type"": ""\""post\"""", ""id"": ""\""twh9v4\"""", ""subreddit.id"": ""\""2sxhs\"""", ""subreddit.name"": ""\""place\"""", ""subreddit.nsfw"": ""false"", ""created_utc"": ""1649116799"", ""permalink"": ""\""https://old.reddit.com/r/place/comments/twh9v4/is..."", ""domain"": ""\""i.redd.it\"""", ""url"": ""\""https://i.redd.it/0kyey4qeplr81.jpg\"""", ""selftext"": ""\""\"""", ""title"": ""\""Is this a glitch? What is up with r/place?\"""", ""score"": ""8""}.

 

	[c] comments
: The columns in this config are type, id, subreddit_id, subreddit_name, subreddit_nsfw, created_utc, permalink, body, sentiment, score.
 An example row from this config is {""type"": ""1"", ""id"": ""\""i3f9n12\"""", ""subreddit.id"": ""\""2sxhs\"""", ""subreddit.name"": ""\""place\"""", ""subreddit.nsfw"": ""false"", ""created_utc"": ""1649116799"", ""permalink"": ""\""https://old.reddit.com/r/place/comments/twdn7y/sp..."", ""body"": ""\""[removed]\"""", ""sentiment"": ""null"", ""score"": ""1""}.

 




[9] yulongmannlp/dev_para
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[10] yulongmannlp/dev_orig
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[11] yulongmannlp/adv_para
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[12] yulongmannlp/adv_ori
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[13] squad
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[14] lhoestq/squad
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[15] Yulong-W/squadpararobustness
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[519]""}.

 




[16] Yulong-W/squadpara
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[17] Yulong-W/squadorirobustness
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[519]""}.

 




[18] Yulong-W/squadori
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 




[19] MajdTannous/Test3
: Description-.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are Unnamed: 0, context, question, id, answer_start, text.
 An example row from this config is {""Unnamed: 0"": ""0"", ""context"": ""\""Beyonc\\u00e9 Giselle Knowles-Carter (/bi\\u02d0\\u0..."", ""question"": ""\""When did Beyonce start becoming popular?\"""", ""id"": ""\""56be85543aeaaa14008c9063\"""", ""answer_start"": ""269"", ""text"": ""\""in the late 1990s\""""}.

 




[20] MajdTannous/Test2
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""56be85543aeaaa14008c9063\"""", ""title"": ""\""Beyonc\\u00e9\"""", ""context"": ""\""Beyonc\\u00e9 Giselle Knowles-Carter (/bi\\u02d0\\u0..."", ""question"": ""\""When did Beyonce start becoming popular?\"""", ""answers.text"": ""[\""in the late 1990s\""]"", ""answers.answer_start"": ""[269]""}.

 




[21] BerMaker/test
: Description-Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are id, title, context, question, answers_text, answers_answer_start.
 An example row from this config is {""id"": ""\""5733be284776f41900661182\"""", ""title"": ""\""University_of_Notre_Dame\"""", ""context"": ""\""Architecturally, the school has a Catholic charac..."", ""question"": ""\""To whom did the Virgin Mary allegedly appear in 1..."", ""answers.text"": ""[\""Saint Bernadette Soubirous\""]"", ""answers.answer_start"": ""[515]""}.

 





The reranking results of the 21 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,jakartaresearch/news-title-gen,default
task211_logic2text_classification,"In this task, you are given commands (in terms of logical operations) and natural interpretation of the given command to select relevant rows from the given table. Your job is to generate a label \""yes\"" if the interpretation is appropriate for the command, otherwise generate label \""no\"". \n Here are the definitions of logical operators: \n 1. count: returns the number of rows in the view. \n 2. only: returns whether there is exactly one row in the view. \n 3. hop: returns the value under the header column of the row. \n 4. and: returns the boolean operation result of two arguments. \n 5. max/min/avg/sum: returns the max/min/average/sum of the values under the header column. \n 6. nth_max/nth_min: returns the n-th max/n-th min of the values under the header column. \n 7. argmax/argmin: returns the row with the max/min value in header column. \n 8. nth_argmax/nth_argmin: returns the row with the n-th max/min value in header column. \n 9. eq/not_eq: returns if the two arguments are equal. \n 10. round_eq: returns if the two arguments are roughly equal under certain tolerance. \n 11. greater/less: returns if the first argument is greater/less than the second argument. \n 12. diff: returns the difference between two arguments. \n 13. filter_eq/ filter_not_eq: returns the subview whose values under the header column is equal/not equal to the third argument. \n 14. filter_greater/filter_less: returns the subview whose values under the header column is greater/less than the third argument. \n 15. filter_greater_eq /filter_less_eq: returns the subview whose values under the header column is greater/less or equal than the third argument. \n 16. filter_all: returns the view itself for the case of describing the whole table \n 17. all_eq/not_eq: returns whether all the values under the header column are equal/not equal to the third argument. \n 18. all_greater/less: returns whether all the values under the header column are greater/less than the third argument. \n 19. all_greater_eq/less_eq: returns whether all the values under the header column are greater/less or equal to the third argument. \n 20. most_eq/not_eq: returns whether most of the values under the header column are equal/not equal to the third argument. \n 21. most_greater/less: returns whether most of the values under the header column are greater/less than the third argument. \n 22. most_greater_eq/less_eq: returns whether most of the values under the header column are greater/less or equal to the third argument.
       {
            ""input"": ""Command: eq { hop { nth_argmax { all_rows ; attendance ; 3 } ; competition } ; danish superliga 2005 - 06 }, interpretation: select the row whose attendance record of all rows is 3rd maximum. the competition record of this row is danish superliga 2005-06."",
            ""output"": ""yes"",
            ""explanation"": ""Here, the command and interpretion given for the command is correct that 3rd maximum should be selected from given table rows. Hence, the label is 'yes'.""
        },
        {
            ""input"": ""Command: eq { hop { argmax { all_rows ; duration } ; actor } ; lesley saweard }, interpretation: select the row whose duration record of all rows is maximum. the actor record of this row is lesley saweard."",
            ""output"": ""yes"",
            ""explanation"": ""Here, the command and interpretion given for the command is correct that the maximum duration should be selected from the given table rows. Hence, the label is 'yes'.""
        },
        {
            ""input"": ""Command: most_eq { all_rows ; points ; 0 }, interpretation: select the row whose duration record of all rows is maximum. the actor record of this row is lesley saweard."",
            ""output"": ""no"",
            ""explanation"": ""Here, the command and interpretion given for the command is not correct because command indicates that the most of the point records are zero, however, interetation indicates that the maximum duration should be selected from the given table rows. Hence, the label is 'no'.""
        },",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 In this task, you are given commands (in terms of logical operations) and natural interpretation of the given command to select relevant rows from the given table. Your job is to generate a label ""yes"" if the interpretation is appropriate for the command, otherwise generate label ""no"". 
 Here are the definitions of logical operators: 
 1. count: returns the number of rows in the view. 
 2. only: returns whether there is exactly one row in the view. 
 3. hop: returns the value under the header column of the row. 
 4. and: returns the boolean operation result of two arguments. 
 5. max/min/avg/sum: returns the max/min/average/sum of the values under the header column. 
 6. nth_max/nth_min: returns the n-th max/n-th min of the values under the header column. 
 7. argmax/argmin: returns the row with the max/min value in header column. 
 8. nth_argmax/nth_argmin: returns the row with the n-th max/min value in header column. 
 9. eq/not_eq: returns if the two arguments are equal. 
 10. round_eq: returns if the two arguments are roughly equal under certain tolerance. 
 11. greater/less: returns if the first argument is greater/less than the second argument. 
 12. diff: returns the difference between two arguments. 
 13. filter_eq/ filter_not_eq: returns the subview whose values under the header column is equal/not equal to the third argument. 
 14. filter_greater/filter_less: returns the subview whose values under the header column is greater/less than the third argument. 
 15. filter_greater_eq /filter_less_eq: returns the subview whose values under the header column is greater/less or equal than the third argument. 
 16. filter_all: returns the view itself for the case of describing the whole table 
 17. all_eq/not_eq: returns whether all the values under the header column are equal/not equal to the third argument. 
 18. all_greater/less: returns whether all the values under the header column are greater/less than the third argument. 
 19. all_greater_eq/less_eq: returns whether all the values under the header column are greater/less or equal to the third argument. 
 20. most_eq/not_eq: returns whether most of the values under the header column are equal/not equal to the third argument. 
 21. most_greater/less: returns whether most of the values under the header column are greater/less than the third argument. 
 22. most_greater_eq/less_eq: returns whether most of the values under the header column are greater/less or equal to the third argument. and these are some examples of the same: Command: eq { hop { nth_argmax { all_rows ; attendance ; 3 } ; competition } ; danish superliga 2005 - 06 }, interpretation: select the row whose attendance record of all rows is 3rd maximum. the competition record of this row is danish superliga 2005-06.

Command: eq { hop { argmax { all_rows ; duration } ; actor } ; lesley saweard }, interpretation: select the row whose duration record of all rows is maximum. the actor record of this row is lesley saweard.

Command: most_eq { all_rows ; points ; 0 }, interpretation: select the row whose duration record of all rows is maximum. the actor record of this row is lesley saweard. 

There are 7 datasets available for this task, each indicated by number identifier []. 

[1] JeremyAlain/123_test
: Description-The Fewshot Table dataset consists of tables that naturally occur on the web, that are formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. The dataset consists of approximately 413K tables that are extracted from the WDC Web Table Corpora 2015, which is released under the Apache-2.0 license. The WDC Web Table Corpora ""contains vast amounts of HTML tables. [...] The Web Data Commons project extracts relational Web tables from the Common Crawl, the largest and most up-to-date Web corpus that is currently available to the public.""
.
. This dataset has the following configs:
  

	[b] data_0
: The columns in this config are task, input, output, options, pageTitle, outputColName, url, wdcFile.
 An example row from this config is {""task"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""input"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""output"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""options"": ""[[\""1\"", \""2\""], [\""1\"", \""2\""], [\""1\"", \""2\""], [\""1\"", \""2\""], [..."", ""pageTitle"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""outputColName"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""url"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""wdcFile"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\""""}.

 

	[c] data_1
: The columns in this config are task, input, output, options, pageTitle, outputColName, url, wdcFile.
 An example row from this config is {""task"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""input"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""output"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""options"": ""[[\""1\"", \""2\""], [\""1\"", \""2\""], [\""1\"", \""2\""], [\""1\"", \""2\""], [..."", ""pageTitle"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""outputColName"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""url"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""wdcFile"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\""""}.

 

	[d] data_2
: The columns in this config are task, input, output, options, pageTitle, outputColName, url, wdcFile.
 An example row from this config is {""task"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""input"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""output"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""options"": ""[[\""1\"", \""2\""], [\""1\"", \""2\""], [\""1\"", \""2\""], [\""1\"", \""2\""], [..."", ""pageTitle"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""outputColName"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""url"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\"""", ""wdcFile"": ""\""[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\""""}.

 




[2] scan
: Description-SCAN tasks with various splits.

SCAN is a set of simple language-driven navigation tasks for studying
compositional learning and zero-shot generalization.

See https://github.com/brendenlake/SCAN for a description of the splits.

Example usage:
data = datasets.load_dataset('scan/length')
.
. This dataset has the following configs:
  

	[b] filler_num1
: The columns in this config are commands, actions.
 An example row from this config is {""commands"": ""\""walk opposite right thrice after run opposite rig..."", ""actions"": ""\""I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TU...""}.

 

	[c] filler_num3
: The columns in this config are commands, actions.
 An example row from this config is {""commands"": ""\""walk opposite right thrice after run opposite rig..."", ""actions"": ""\""I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TU...""}.

 

	[d] simple
: The columns in this config are commands, actions.
 An example row from this config is {""commands"": ""\""jump opposite right twice and turn opposite right..."", ""actions"": ""\""I_TURN_RIGHT I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_T...""}.

 

	[e] filler_num2
: The columns in this config are commands, actions.
 An example row from this config is {""commands"": ""\""walk opposite right thrice after run opposite rig..."", ""actions"": ""\""I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_TU...""}.

 

	[f] addprim_jump
: The columns in this config are commands, actions.
 An example row from this config is {""commands"": ""\""jump\"""", ""actions"": ""\""I_JUMP\""""}.

 




[3] code_x_glue_cc_code_refinement
: Description-CodeXGLUE code-refinement dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/code-refinement

We use the dataset released by this paper(https://arxiv.org/pdf/1812.08693.pdf). The source side is a Java function with bugs and the target side is the refined one. All the function and variable names are normalized. Their dataset contains two subsets ( i.e.small and medium) based on the function length..
. This dataset has the following configs:
  

	[b] medium
: The columns in this config are id, buggy, fixed.
 An example row from this config is {""id"": ""0"", ""buggy"": ""\""public static TYPE_1 init ( java.lang.String name..."", ""fixed"": ""\""public static TYPE_1 init ( java.lang.String name...""}.

 

	[c] small
: The columns in this config are id, buggy, fixed.
 An example row from this config is {""id"": ""0"", ""buggy"": ""\""public java.lang.String METHOD_1 ( ) { return new..."", ""fixed"": ""\""public java.lang.String METHOD_1 ( ) { return new...""}.

 




[4] clinc_oos
: Description-    This dataset is for evaluating the performance of intent classification systems in the
    presence of ""out-of-scope"" queries. By ""out-of-scope"", we mean queries that do not fall
    into any of the system-supported intent classes. Most datasets include only data that is
    ""in-scope"". Our dataset includes both in-scope and out-of-scope data. You might also know
    the term ""out-of-scope"" by other terms, including ""out-of-domain"" or ""out-of-distribution"".

Small, in which there are only 50 training queries per each in-scope intent
.
. This dataset has the following configs:
  

	[b] small
: The columns in this config are text, intent.
 An example row from this config is {""text"": ""\""can you walk me through setting up direct deposit..."", ""intent"": ""108""}.

 

	[c] imbalanced
: The columns in this config are text, intent.
 An example row from this config is {""text"": ""\""what are the steps for setting up direct deposit ..."", ""intent"": ""108""}.

 

	[d] plus
: The columns in this config are text, intent.
 An example row from this config is {""text"": ""\""what expression would i use to say i love you if ..."", ""intent"": ""61""}.

 




[5] dart
: Description-DART is a large and open-domain structured DAta Record to Text generation corpus with high-quality
sentence annotations with each input being a set of entity-relation triples following a tree-structured ontology.
It consists of 82191 examples across different domains with each input being a semantic RDF triple set derived
from data records in tables and the tree ontology of table schema, annotated with sentence description that
covers all facts in the triple set.

DART is released in the following paper where you can find more details and baseline results:
https://arxiv.org/abs/2007.02871
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are tripleset, subtree_was_extended, annotations_source, annotations_text.
 An example row from this config is {""tripleset"": ""[[\""First Clearing\"", \""LOCATION\"", \""On NYS 52 1 Mi. Y..."", ""subtree_was_extended"": ""false"", ""annotations.source"": ""[\""WikiTableQuestions_mturk\""]"", ""annotations.text"": ""[\""First Clearing\\tbased on Callicoon, New York and...""}.

 




[6] sede
: Description-SEDE (Stack Exchange Data Explorer) is new dataset for Text-to-SQL tasks with more than 12,000 SQL queries and their
natural language description. It's based on a real usage of users from the Stack Exchange Data Explorer platform,
which brings complexities and challenges never seen before in any other semantic parsing dataset like
including complex nesting, dates manipulation, numeric and text manipulation, parameters, and most
importantly: under-specification and hidden-assumptions.

Paper (NLP4Prog workshop at ACL2021): https://arxiv.org/abs/2106.05006
.
. This dataset has the following configs:
  

	[b] sede
: The columns in this config are QuerySetId, Title, Description, QueryBody, CreationDate, validated.
 An example row from this config is {""QuerySetId"": ""466"", ""Title"": ""\""Most controversial posts on the site\"""", ""Description"": ""\""Looks for posts with more than half the amount of..."", ""QueryBody"": ""\""SELECT \\n* from Votes\"""", ""CreationDate"": ""\""2020-06-24 11:23:10\"""", ""validated"": ""false""}.

 




[7] metaeval/utilitarianism
: Description-.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are better_choice, worst_choice, comparison, label.
 An example row from this config is {""better_choice"": ""\""I built a sandcastle with my nephew. We made one ..."", ""worst_choice"": ""\""I built a sandcastle with my nephew\"""", ""comparison"": ""\""\\\""I built a sandcastle with my nephew. We made on..."", ""label"": ""1""}.

 





The reranking results of the 7 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,sede,sede
task369_synthetic_remove_odds.json,"In this task, you will be given a list of integers. You should remove all of the odd integers from the list(consider 0 an even number). If every integer in the input list is odd then an empty list (\""[]\"") should be returned. Otherwise, answer with the list of even numbers separated by comma inside brackets.
        {
            ""input"": ""[1, 8, 0, 2, 9]"",
            ""output"": ""[8, 0, 2]"",
            ""explanation"": ""1 and 9 are removed from the list because they are odd numbers.""
        },
        {
            ""input"": ""[1, 3, 7, 1]"",
            ""output"": ""[]"",
            ""explanation"": ""Every value in the input list is odd so an empty list is returned.""
        },
        {
            ""input"": ""[-3, -10, -31, -2]"",
            ""output"": ""[-10, -2]"",
            ""explanation"": ""-3 and -31 are removed from the list, because they are odd numbers.""
        }",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 In this task, you will be given a list of integers. You should remove all of the odd integers from the list(consider 0 an even number). If every integer in the input list is odd then an empty list (""[]"") should be returned. Otherwise, answer with the list of even numbers separated by comma inside brackets. and these are some examples of the same: {
            ""input"": ""[1, 8, 0, 2, 9]"",
            ""output"": ""[8, 0, 2]"",
            ""explanation"": ""1 and 9 are removed from the list because they are odd numbers.""
        },
        {
            ""input"": ""[1, 3, 7, 1]"",
            ""output"": ""[]"",
            ""explanation"": ""Every value in the input list is odd so an empty list is returned.""
        },
        {
            ""input"": ""[-3, -10, -31, -2]"",
            ""output"": ""[-10, -2]"",
            ""explanation"": ""-3 and -31 are removed from the list, because they are odd numbers.""
        } 

There are 12 datasets available for this task, each indicated by number identifier []. 

[1] enwik8
: Description-The dataset is based on the Hutter Prize (http://prize.hutter1.net) and contains the first 10^8 bytes of English Wikipedia in 2006 in XML
.
. This dataset has the following configs:
  

	[b] enwik8
: The columns in this config are text.
 An example row from this config is {""text"": ""\""<mediawiki xmlns=\\\""http://www.mediawiki.org/xml/e...""}.

 

	[c] enwik8-raw
: The columns in this config are text.
 An example row from this config is {""text"": ""\""<mediawiki xmlns=\\\""http://www.mediawiki.org/xml/e...""}.

 




[2] dbpedia_14
: Description-The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes
from DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we
randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size
of the training dataset is 560,000 and testing dataset 70,000.
There are 3 columns in the dataset (same for train and test splits), corresponding to class index
(1 to 14), title and content. The title and content are escaped using double quotes (""), and any
internal double quote is escaped by 2 double quotes (""""). There are no new lines in title or content.
.
. This dataset has the following configs:
  

	[b] dbpedia_14
: The columns in this config are label, title, content.
 An example row from this config is {""label"": ""0"", ""title"": ""\""E. D. Abbott Ltd\"""", ""content"": ""\"" Abbott of Farnham E D Abbott Limited was a Briti...""}.

 




[3] yuanchuan/annotated_reference_strings
: Description-A repository of reference strings annotated using CSL processor using citations obtained from various sources.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are source, lang, entry_type, doi_prefix, csl_style, content.
 An example row from this config is {""source"": ""\""crossref\"""", ""lang"": ""\""en\"""", ""entry_type"": ""\""article\"""", ""doi_prefix"": ""\""10.1021\"""", ""csl_style"": ""\""nature\"""", ""content"": ""\""<citation-number>1.</citation-number> <author>She...""}.

 




[4] codeparrot/apps
: Description-APPS is a benchmark for Python code generation, it includes 10,000 problems, which range from having simple oneline solutions to being substantial algorithmic challenges, for more details please refer to this paper: https://arxiv.org/pdf/2105.09938.pdf.
.
. This dataset has the following configs:
  

	[b] all
: The columns in this config are problem_id, question, solutions, input_output, difficulty, url, starter_code.
 An example row from this config is {""problem_id"": ""0"", ""question"": ""\""Polycarp has $n$ different binary words. A word c..."", ""solutions"": ""\""[\\\""for _ in range(int(input())):\\\\n    n = int(in..."", ""input_output"": ""\""{\\n  \\\""inputs\\\"": [\\n    \\\""4\\\\n4\\\\n0001\\\\n1000\\\\n0..."", ""difficulty"": ""\""interview\"""", ""url"": ""\""https://codeforces.com/problemset/problem/1259/D\""..."", ""starter_code"": ""\""\""""}.

 

	[c] introductory
: The columns in this config are problem_id, question, solutions, input_output, difficulty, url, starter_code.
 An example row from this config is {""problem_id"": ""2361"", ""question"": ""\""You are given an array $a$ of length $n$ consisti..."", ""solutions"": ""\""[\\\""from collections import defaultdict as dd\\\\nfr..."", ""input_output"": ""\""{\\\""inputs\\\"": [\\\""6\\\\n1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n\\\""], ..."", ""difficulty"": ""\""introductory\"""", ""url"": ""\""https://codeforces.com/problemset/problem/1353/D\""..."", ""starter_code"": ""\""\""""}.

 

	[d] interview
: The columns in this config are problem_id, question, solutions, input_output, difficulty, url, starter_code.
 An example row from this config is {""problem_id"": ""0"", ""question"": ""\""Polycarp has $n$ different binary words. A word c..."", ""solutions"": ""\""[\\\""for _ in range(int(input())):\\\\n    n = int(in..."", ""input_output"": ""\""{\\n  \\\""inputs\\\"": [\\n    \\\""4\\\\n4\\\\n0001\\\\n1000\\\\n0..."", ""difficulty"": ""\""interview\"""", ""url"": ""\""https://codeforces.com/problemset/problem/1259/D\""..."", ""starter_code"": ""\""\""""}.

 

	[e] competition
: The columns in this config are problem_id, question, solutions, input_output, difficulty, url, starter_code.
 An example row from this config is {""problem_id"": ""2000"", ""question"": ""\""Codefortia is a small island country located some..."", ""solutions"": ""\""[\\\""import heapq\\\\nn,m,a,b=map(int,input().split()..."", ""input_output"": ""\""{\\n  \\\""inputs\\\"": [\\n    \\\""5 5 20 25\\\\n1 2 25\\\\n2 ..."", ""difficulty"": ""\""competition\"""", ""url"": ""\""https://codeforces.com/problemset/problem/1149/D\""..."", ""starter_code"": ""\""\""""}.

 




[5] sst
: Description-The Stanford Sentiment Treebank, the first corpus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of sentiment in language.
.
. This dataset has the following configs:
  

	[b] ptb
: The columns in this config are ptb_tree.
 An example row from this config is {""ptb_tree"": ""\""(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destin...""}.

 




[6] vesteinn/swe-nerc
: Description-The corpus consists of ca. 150.000 words of text.
.
. This dataset has the following configs:
  

	[b] swe-nerc
: The columns in this config are id, tokens, ner_tags.
 An example row from this config is {""id"": ""\""0\"""", ""tokens"": ""[\""Det\"", \""har\"", \""iaf\"", \""jag\"", \""gjort\"", \""men\"", \""ska\""..."", ""ner_tags"": ""[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...""}.

 




[7] cfilt/iwn_wordlists
: Description-We provide the unique word list form the IndoWordnet (IWN) knowledge base.
.
. This dataset has the following configs:
  

	[b] nepali
: The columns in this config are word.
 An example row from this config is {""word"": ""\""\\u0906\\u0930\\u094b\\u092a\\u0923\""""}.

 

	[c] manipuri
: The columns in this config are word.
 An example row from this config is {""word"": ""\""mmL_yAMlb \""""}.

 

	[d] sanskrit
: The columns in this config are word.
 An example row from this config is {""word"": ""\""\\u092d\\u0930\\u0926\\u094d\\u0935\\u093e\\u091c\\u0903\""...""}.

 

	[e] meitei
: The columns in this config are word.
 An example row from this config is {""word"": ""\""\\uabc3\\uabe6\\uabdb\\uabc1\\uabe4\\uabc0\\uabe3\\uabed\\...""}.

 

	[f] telugu
: The columns in this config are word.
 An example row from this config is {""word"": ""\""\\u0c28\\u0c2e\\u0c4d\\u0c2e\\u0c26\\u0c17\\u0c3f\\u0c28\""...""}.

 




[8] sst2
: Description-The Stanford Sentiment Treebank consists of sentences from movie reviews and
human annotations of their sentiment. The task is to predict the sentiment of a
given sentence. We use the two-way (positive/negative) class split, and use only
sentence-level labels.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are idx, sentence, label.
 An example row from this config is {""idx"": ""0"", ""sentence"": ""\""hide new secretions from the parental units \"""", ""label"": ""0""}.

 




[9] maritaca-ai/sst2_pt
: Description-The Stanford Sentiment Treebank consists of sentences from movie reviews and
human annotations of their sentiment. The task is to predict the sentiment of a
given sentence. We use the two-way (positive/negative) class split, and use only
sentence-level labels.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""esconder novas secre\\u00e7\\u00f5es das unidades p..."", ""label"": ""0""}.

 




[10] mideind/icelandic-error-corpus-IceEC
: Description-The Icelandic Error Corpus (IceEC) is a collection of texts in modern Icelandic annotated for mistakes related to spelling, grammar, and other issues. The texts are organized by genre. The current version includes sentences from student essays, online news texts and Wikipedia articles.
Sentences within texts in the student essays had to be shuffled due to the license which they were originally published under, but neither the online news texts nor the Wikipedia articles needed to be shuffled.
.
. This dataset has the following configs:
  

	[b] fine-grained
: The columns in this config are idx, sentence, errors, has_error, corrected_sentence.
 An example row from this config is {""idx"": ""\""0\"""", ""sentence"": ""[\""Jack\"", \""Live\"", \""kv\\u00f6ldin\"", \""halda\"", \""g\\u00f6..."", ""errors"": ""[[], [], [], [], [], [], [], [], [], [], [], [], [..."", ""has_error"": ""false"", ""corrected_sentence"": ""[\""Jack\"", \""Live\"", \""kv\\u00f6ldin\"", \""halda\"", \""g\\u00f6...""}.

 

	[c] subcategory
: The columns in this config are idx, sentence, errors, has_error, corrected_sentence.
 An example row from this config is {""idx"": ""\""0\"""", ""sentence"": ""[\""Jack\"", \""Live\"", \""kv\\u00f6ldin\"", \""halda\"", \""g\\u00f6..."", ""errors"": ""[[], [], [], [], [], [], [], [], [], [], [], [], [..."", ""has_error"": ""false"", ""corrected_sentence"": ""[\""Jack\"", \""Live\"", \""kv\\u00f6ldin\"", \""halda\"", \""g\\u00f6...""}.

 

	[d] category
: The columns in this config are idx, sentence, errors, has_error, corrected_sentence.
 An example row from this config is {""idx"": ""\""0\"""", ""sentence"": ""[\""Jack\"", \""Live\"", \""kv\\u00f6ldin\"", \""halda\"", \""g\\u00f6..."", ""errors"": ""[[], [], [], [], [], [], [], [], [], [], [], [], [..."", ""has_error"": ""false"", ""corrected_sentence"": ""[\""Jack\"", \""Live\"", \""kv\\u00f6ldin\"", \""halda\"", \""g\\u00f6...""}.

 




[11] code_search_net
: Description-CodeSearchNet corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation.
.
. This dataset has the following configs:
  

	[b] java
: The columns in this config are repository_name, func_path_in_repository, func_name, whole_func_string, language, func_code_string, func_code_tokens, func_documentation_string, func_documentation_tokens, split_name, func_code_url.
 An example row from this config is {""repository_name"": ""\""spring-projects/spring-boot\"""", ""func_path_in_repository"": ""\""spring-boot-project/spring-boot/src/main/java/org..."", ""func_name"": ""\""IndexedElementsBinder.bindIndexed\"""", ""whole_func_string"": ""\""protected final void bindIndexed(ConfigurationPro..."", ""language"": ""\""java\"""", ""func_code_string"": ""\""protected final void bindIndexed(ConfigurationPro..."", ""func_code_tokens"": ""[\""protected\"", \""final\"", \""void\"", \""bindIndexed\"", \""(\"",..."", ""func_documentation_string"": ""\""Bind indexed elements to the supplied collection...."", ""func_documentation_tokens"": ""[\""Bind\"", \""indexed\"", \""elements\"", \""to\"", \""the\"", \""supp..."", ""split_name"": ""\""train\"""", ""func_code_url"": ""\""https://github.com/spring-projects/spring-boot/bl...""}.

 

	[c] javascript
: The columns in this config are repository_name, func_path_in_repository, func_name, whole_func_string, language, func_code_string, func_code_tokens, func_documentation_string, func_documentation_tokens, split_name, func_code_url.
 An example row from this config is {""repository_name"": ""\""Microsoft/vscode\"""", ""func_path_in_repository"": ""\""build/lib/treeshaking.js\"""", ""func_name"": ""\""createTypeScriptLanguageService\"""", ""whole_func_string"": ""\""function createTypeScriptLanguageService(options)..."", ""language"": ""\""javascript\"""", ""func_code_string"": ""\""function createTypeScriptLanguageService(options)..."", ""func_code_tokens"": ""[\""function\"", \""createTypeScriptLanguageService\"", \""(..."", ""func_documentation_string"": ""\""#region Discovery, LanguageService & Setup\"""", ""func_documentation_tokens"": ""[\""#region\"", \""Discovery\"", \""LanguageService\"", \""&\"", \""..."", ""split_name"": ""\""train\"""", ""func_code_url"": ""\""https://github.com/Microsoft/vscode/blob/693a13cd...""}.

 

	[d] php
: The columns in this config are repository_name, func_path_in_repository, func_name, whole_func_string, language, func_code_string, func_code_tokens, func_documentation_string, func_documentation_tokens, split_name, func_code_url.
 An example row from this config is {""repository_name"": ""\""domnikl/DesignPatternsPHP\"""", ""func_path_in_repository"": ""\""Structural/Registry/Registry.php\"""", ""func_name"": ""\""Registry.set\"""", ""whole_func_string"": ""\""public static function set(string $key, $value)\\n..."", ""language"": ""\""php\"""", ""func_code_string"": ""\""public static function set(string $key, $value)\\n..."", ""func_code_tokens"": ""[\""public\"", \""static\"", \""function\"", \""set\"", \""(\"", \""stri..."", ""func_documentation_string"": ""\""@param string $key\\n@param mixed  $value\\n\\n@retu..."", ""func_documentation_tokens"": ""[\""@param\"", \""string\"", \""$key\"", \""@param\"", \""mixed\"", \""$..."", ""split_name"": ""\""train\"""", ""func_code_url"": ""\""https://github.com/domnikl/DesignPatternsPHP/blob...""}.

 

	[e] go
: The columns in this config are repository_name, func_path_in_repository, func_name, whole_func_string, language, func_code_string, func_code_tokens, func_documentation_string, func_documentation_tokens, split_name, func_code_url.
 An example row from this config is {""repository_name"": ""\""kubernetes/kubernetes\"""", ""func_path_in_repository"": ""\""staging/src/k8s.io/apimachinery/pkg/runtime/exten..."", ""func_name"": ""\""MarshalJSON\"""", ""whole_func_string"": ""\""func (re RawExtension) MarshalJSON() ([]byte, err..."", ""language"": ""\""go\"""", ""func_code_string"": ""\""func (re RawExtension) MarshalJSON() ([]byte, err..."", ""func_code_tokens"": ""[\""func\"", \""(\"", \""re\"", \""RawExtension\"", \"")\"", \""MarshalJ..."", ""func_documentation_string"": ""\""// MarshalJSON may get called on pointers or valu..."", ""func_documentation_tokens"": ""[\""MarshalJSON\"", \""may\"", \""get\"", \""called\"", \""on\"", \""poi..."", ""split_name"": ""\""train\"""", ""func_code_url"": ""\""https://github.com/kubernetes/kubernetes/blob/6a8...""}.

 

	[f] all
: The columns in this config are repository_name, func_path_in_repository, func_name, whole_func_string, language, func_code_string, func_code_tokens, func_documentation_string, func_documentation_tokens, split_name, func_code_url.
 An example row from this config is {""repository_name"": ""\""ageitgey/face_recognition\"""", ""func_path_in_repository"": ""\""examples/face_recognition_knn.py\"""", ""func_name"": ""\""train\"""", ""whole_func_string"": ""\""def train(train_dir, model_save_path=None, n_neig..."", ""language"": ""\""python\"""", ""func_code_string"": ""\""def train(train_dir, model_save_path=None, n_neig..."", ""func_code_tokens"": ""[\""def\"", \""train\"", \""(\"", \""train_dir\"", \"",\"", \""model_sav..."", ""func_documentation_string"": ""\""Trains a k-nearest neighbors classifier for face ..."", ""func_documentation_tokens"": ""[\""Trains\"", \""a\"", \""k\"", \""-\"", \""nearest\"", \""neighbors\"", ..."", ""split_name"": ""\""train\"""", ""func_code_url"": ""\""https://github.com/ageitgey/face_recognition/blob...""}.

 




[12] wiki_qa
: Description-Wiki Question Answering corpus from Microsoft
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are question_id, question, document_title, answer, label.
 An example row from this config is {""question_id"": ""\""Q1\"""", ""question"": ""\""how are glacier caves formed?\"""", ""document_title"": ""\""Glacier cave\"""", ""answer"": ""\""A partly submerged glacier cave on Perito Moreno ..."", ""label"": ""0""}.

 





The reranking results of the 12 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,code_search_net,all
task326_jigsaw_classification_obscene,"""In this task, you are given a public comment from online platforms. You are expected to classify the comment into two classes: obscene and non-obscene. A comment is obscene if it is considered offensive to the public sense of decency, for example because it too obviously relates to sex or contains language regarded as taboo in polite usage.
     {
            ""input"": ""Comment: Damn!  Another cloudy day, and it's all Obama's doing!"",
            ""output"": ""Obscene"",
            ""explanation"": ""This is obscenity because it hints on sexual content that is offensive to the public sense of decency.""
        },
        {
            ""input"": ""Comment: Be a victim or shoot back."",
            ""output"": ""Non-obscene"",
            ""explanation"": ""This is unrelated to pornography or obscenity.""
        }",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 ""In this task, you are given a public comment from online platforms. You are expected to classify the comment into two classes: obscene and non-obscene. A comment is obscene if it is considered offensive to the public sense of decency, for example because it too obviously relates to sex or contains language regarded as taboo in polite usage. and these are some examples of the same: {
            ""input"": ""Comment: Damn!  Another cloudy day, and it's all Obama's doing!"",
            ""output"": ""Obscene"",
            ""explanation"": ""This is obscenity because it hints on sexual content that is offensive to the public sense of decency.""
        },
        {
            ""input"": ""Comment: Be a victim or shoot back."",
            ""output"": ""Non-obscene"",
            ""explanation"": ""This is unrelated to pornography or obscenity.""
        } 

There are 20 datasets available for this task, each indicated by number identifier []. 

[1] ethos
: Description-.
. This dataset has the following configs:
  

	[b] binary
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""You should know women's sports are a joke\"""", ""label"": ""1""}.

 

	[c] multilabel
: The columns in this config are text, violence, directed_vs_generalized, gender, race, national_origin, disability, religion, sexual_orientation.
 An example row from this config is {""text"": ""\""You should know women's sports are a joke\"""", ""violence"": ""0"", ""directed_vs_generalized"": ""0"", ""gender"": ""1"", ""race"": ""0"", ""national_origin"": ""0"", ""disability"": ""0"", ""religion"": ""0"", ""sexual_orientation"": ""0""}.

 




[2] SetFit/ethos
: Description-.
. This dataset has the following configs:
  

	[b] binary
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""You should know women's sports are a joke\"""", ""label"": ""1""}.

 

	[c] multilabel
: The columns in this config are text, violence, directed_vs_generalized, gender, race, national_origin, disability, religion, sexual_orientation.
 An example row from this config is {""text"": ""\""You should know women's sports are a joke\"""", ""violence"": ""0"", ""directed_vs_generalized"": ""0"", ""gender"": ""1"", ""race"": ""0"", ""national_origin"": ""0"", ""disability"": ""0"", ""religion"": ""0"", ""sexual_orientation"": ""0""}.

 




[3] cdt
: Description-The Cyberbullying Detection task was part of 2019 edition of PolEval competition. The goal is to predict if a given Twitter message contains a cyberbullying (harmful) content.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are sentence, target.
 An example row from this config is {""sentence"": ""\""Dla mnie faworytem do tytu\\u0142u b\\u0119dzie Cra..."", ""target"": ""0""}.

 




[4] hate_speech_offensive
: Description-An annotated dataset for hate speech and offensive language detection on tweets.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are count, hate_speech_count, offensive_language_count, neither_count, class, tweet.
 An example row from this config is {""count"": ""3"", ""hate_speech_count"": ""0"", ""offensive_language_count"": ""0"", ""neither_count"": ""3"", ""class"": ""2"", ""tweet"": ""\""!!! RT @mayasolovely: As a woman you shouldn't co...""}.

 




[5] tweets_hate_speech_detection
: Description-The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.

Formally, given a training sample of tweets and labels, where label ‘1’ denotes the tweet is racist/sexist and label ‘0’ denotes the tweet is not racist/sexist, your objective is to predict the labels on the given test dataset.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are label, tweet.
 An example row from this config is {""label"": ""0"", ""tweet"": ""\""@user when a father is dysfunctional and is so se...""}.

 




[6] peixian/equity_evaluation_corpus
: Description-Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems and resources. Further, there is a lack of benchmark datasets for examining inappropriate biases in system predictions. Here, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We used the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 ‘Affect in Tweets’. We found that several of the systems showed statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available, and encourage its use to evaluate biases in sentiment and other NLP tasks.
.
. This dataset has the following configs:
  

	[b] first_domain
: The columns in this config are sentence, template, person, gender, race, emotion, emotion word.
 An example row from this config is {""sentence"": ""\""Alonzo feels angry.\"""", ""template"": ""\""<person subject> feels <emotion word>.\"""", ""person"": ""\""Alonzo\"""", ""gender"": ""\""male\"""", ""race"": ""\""African-American\"""", ""emotion"": ""\""anger\"""", ""emotion word"": ""\""angry\""""}.

 




[7] hebrew_sentiment
: Description-HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel’s
president, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder,
2013) to scrape all the comments to all of the president’s posts in the period of June – August 2014,
the first three months of Rivlin’s presidency.2 While the president’s posts aimed at reconciling tensions
and called for tolerance and empathy, the sentiment expressed in the comments to the president’s posts
was polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his
policy. Of the 12,804 comments, 370 are neutral; 8,512 are positive, 3,922 negative.

Data Annotation: A trained researcher examined each comment and determined its sentiment value,
where comments with an overall positive sentiment were assigned the value 1, comments with an overall
negative sentiment were assigned the value -1, and comments that are off-topic to the post’s content
were assigned the value 0. We validated the coding scheme by asking a second trained researcher to
code the same data. There was substantial agreement between raters (N of agreements: 10623, N of
disagreements: 2105, Coehn’s Kappa = 0.697, p = 0).
.
. This dataset has the following configs:
  

	[b] token
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""\\u05de\\u05de\\u05e9 \\u05db\\u05d5\\u05d0\\u05d1 ........"", ""label"": ""0""}.

 

	[c] morph
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""\\u05de\\u05de\\u05e9 \\u05db\\u05d5\\u05d0\\u05d1 ........"", ""label"": ""0""}.

 




[8] fake_news_english
: Description-
Fake news has become a major societal issue and a technical challenge for social media companies to identify. This content is difficult to identify because the term ""fake news"" covers intentionally false, deceptive stories as well as factual errors, satire, and sometimes, stories that a person just does not like. Addressing the problem requires clear definitions and examples. In this work, we present a dataset of fake news and satire stories that are hand coded, verified, and, in the case of fake news, include rebutting stories. We also include a thematic content analysis of the articles, identifying major themes that include hyperbolic support or condemnation of a gure, conspiracy theories, racist themes, and discrediting of reliable sources. In addition to releasing this dataset for research use, we analyze it and show results based on language that are promising for classification purposes. Overall, our contribution of a dataset and initial analysis are designed to support future work by fake news researchers.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are article_number, url_of_article, fake_or_satire, url_of_rebutting_article.
 An example row from this config is {""article_number"": ""375"", ""url_of_article"": ""\""http://www.redflagnews.com/headlines-2016/cdc-pro..."", ""fake_or_satire"": ""1"", ""url_of_rebutting_article"": ""\""http://www.snopes.com/cdc-forced-vaccinations/\""...""}.

 




[9] hate_speech18
: Description-These files contain text extracted from Stormfront, a white supremacist forum. A random set of
forums posts have been sampled from several subforums and split into sentences. Those sentences
have been manually labelled as containing hate speech or not, according to certain annotation guidelines.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, user_id, subforum_id, num_contexts, label.
 An example row from this config is {""text"": ""\""As of March 13th , 2014 , the booklet had been do..."", ""user_id"": ""572066"", ""subforum_id"": ""1346"", ""num_contexts"": ""0"", ""label"": ""0""}.

 




[10] bn_hate_speech
: Description-The Bengali Hate Speech Dataset is a collection of Bengali articles collected from Bengali news articles,
news dump of Bengali TV channels, books, blogs, and social media. Emphasis was placed on Facebook pages and
newspaper sources because they attract close to 50 million followers and is a common source of opinions
and hate speech. The raw text corpus contains 250 million articles and the full dataset is being prepared
for release. This is a subset of the full dataset.

This dataset was prepared for hate-speech text classification benchmark on Bengali, an under-resourced language.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""\\u0987\\u09a8\\u09bf\\u0987 \\u09b9\\u099a\\u09cd\\u099b..."", ""label"": ""3""}.

 




[11] poleval2019_cyberbullying
: Description-    In Task 6-1, the participants are to distinguish between normal/non-harmful tweets (class: 0) and tweets
    that contain any kind of harmful information (class: 1). This includes cyberbullying, hate speech and
    related phenomena.

    In Task 6-2, the participants shall distinguish between three classes of tweets: 0 (non-harmful),
    1 (cyberbullying), 2 (hate-speech). There are various definitions of both cyberbullying and hate-speech,
    some of them even putting those two phenomena in the same group. The specific conditions on which we based
    our annotations for both cyberbullying and hate-speech, which have been worked out during ten years of research
    will be summarized in an introductory paper for the task, however, the main and definitive condition to 1
    distinguish the two is whether the harmful action is addressed towards a private person(s) (cyberbullying),
    or a public person/entity/large group (hate-speech).
.
. This dataset has the following configs:
  

	[b] task01
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""Dla mnie faworytem do tytu\\u0142u b\\u0119dzie Cra..."", ""label"": ""0""}.

 

	[c] task02
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""Dla mnie faworytem do tytu\\u0142u b\\u0119dzie Cra..."", ""label"": ""0""}.

 




[12] social_bias_frames
: Description-Social Bias Frames is a new way of representing the biases and offensiveness that are implied in language.
For example, these frames are meant to distill the implication that ""women (candidates) are less qualified""
behind the statement ""we shouldn’t lower our standards to hire more women.""
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are whoTarget, intentYN, sexYN, sexReason, offensiveYN, annotatorGender, annotatorMinority, sexPhrase, speakerMinorityYN, WorkerId, HITId, annotatorPolitics, annotatorRace, annotatorAge, post, targetMinority, targetCategory, targetStereotype, dataSource.
 An example row from this config is {""whoTarget"": ""\""0.0\"""", ""intentYN"": ""\""0.66\"""", ""sexYN"": ""\""0.0\"""", ""sexReason"": ""\""\"""", ""offensiveYN"": ""\""1.0\"""", ""annotatorGender"": ""\""woman\"""", ""annotatorMinority"": ""\""\"""", ""sexPhrase"": ""\""\"""", ""speakerMinorityYN"": ""\""\"""", ""WorkerId"": ""\""-8935932304856669427\"""", ""HITId"": ""\""363A7XIFV4G2799C5V96YERJA9AVAM\"""", ""annotatorPolitics"": ""\""liberal\"""", ""annotatorRace"": ""\""white\"""", ""annotatorAge"": ""\""45.0\"""", ""post"": ""\""RT @_LexC__: I'm convinced that some of y'all bit..."", ""targetMinority"": ""\""\"""", ""targetCategory"": ""\""\"""", ""targetStereotype"": ""\""\"""", ""dataSource"": ""\""t/davidson\""""}.

 




[13] liar
: Description-LIAR is a dataset for fake news detection with 12.8K human labeled short statements from politifact.com's API, and each statement is evaluated by a politifact.com editor for its truthfulness. The distribution of labels in the LIAR dataset is relatively well-balanced: except for 1,050 pants-fire cases, the instances for all other labels range from 2,063 to 2,638. In each case, the labeler provides a lengthy analysis report to ground each judgment.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are id, label, statement, subject, speaker, job_title, state_info, party_affiliation, barely_true_counts, false_counts, half_true_counts, mostly_true_counts, pants_on_fire_counts, context.
 An example row from this config is {""id"": ""\""2635.json\"""", ""label"": ""0"", ""statement"": ""\""Says the Annies List political group supports thi..."", ""subject"": ""\""abortion\"""", ""speaker"": ""\""dwayne-bohac\"""", ""job_title"": ""\""State representative\"""", ""state_info"": ""\""Texas\"""", ""party_affiliation"": ""\""republican\"""", ""barely_true_counts"": ""0.0"", ""false_counts"": ""1.0"", ""half_true_counts"": ""0.0"", ""mostly_true_counts"": ""0.0"", ""pants_on_fire_counts"": ""0.0"", ""context"": ""\""a mailer\""""}.

 




[14] event2Mind
: Description-In Event2Mind, we explore the task of understanding stereotypical intents and reactions to events. Through crowdsourcing, we create a large corpus with 25,000 events and free-form descriptions of their intents and reactions, both of the event's subject and (potentially implied) other participants.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are Source, Event, Xintent, Xemotion, Otheremotion, Xsent, Osent.
 An example row from this config is {""Source"": ""\""it_events\"""", ""Event"": ""\""It is PersonY's favorite color\"""", ""Xintent"": ""\""[\\\""none\\\""]\"""", ""Xemotion"": ""\""[\\\""none\\\""]\"""", ""Otheremotion"": ""\""[\\\""happy\\\""]\"""", ""Xsent"": ""\""\"""", ""Osent"": ""\""4.0\""""}.

 




[15] ruanchaves/hatebr
: Description-
HateBR is the first large-scale expert annotated corpus of Brazilian Instagram comments for hate speech and offensive language detection on the web and social media. The HateBR corpus was collected from Brazilian Instagram comments of politicians and manually annotated by specialists. It is composed of 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive comments), offensiveness-level (highly, moderately, and slightly offensive messages), and nine hate speech groups (xenophobia, racism, homophobia, sexism, religious intolerance, partyism, apology for the dictatorship, antisemitism, and fatphobia). Each comment was annotated by three different annotators and achieved high inter-annotator agreement. Furthermore, baseline experiments were implemented reaching 85% of F1-score outperforming the current literature models for the Portuguese language. Accordingly, we hope that the proposed expertly annotated corpus may foster research on hate speech and offensive language detection in the Natural Language Processing area.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are instagram_comments, offensive_language, offensiveness_levels, antisemitism, apology_for_the_dictatorship, fatphobia, homophobia, partyism, racism, religious_intolerance, sexism, xenophobia, offensive_&_non-hate_speech, non-offensive, specialist_1_hate_speech, specialist_2_hate_speech, specialist_3_hate_speech.
 An example row from this config is {""instagram_comments"": ""\""este lixo ...\"""", ""offensive_language"": ""true"", ""offensiveness_levels"": ""1"", ""antisemitism"": ""false"", ""apology_for_the_dictatorship"": ""false"", ""fatphobia"": ""false"", ""homophobia"": ""false"", ""partyism"": ""false"", ""racism"": ""false"", ""religious_intolerance"": ""false"", ""sexism"": ""false"", ""xenophobia"": ""false"", ""offensive_&_non-hate_speech"": ""true"", ""non-offensive"": ""false"", ""specialist_1_hate_speech"": ""false"", ""specialist_2_hate_speech"": ""false"", ""specialist_3_hate_speech"": ""false""}.

 




[16] humicroedit
: Description-This new dataset is designed to assess the funniness of edited news headlines.
.
. This dataset has the following configs:
  

	[b] subtask-1
: The columns in this config are id, original, edit, grades, meanGrade.
 An example row from this config is {""id"": ""\""14530\"""", ""original"": ""\""France is \\u2018 hunting down its citizens who jo..."", ""edit"": ""\""twins\"""", ""grades"": ""\""10000\"""", ""meanGrade"": ""0.2""}.

 

	[c] subtask-2
: The columns in this config are id, original1, edit1, grades1, meanGrade1, original2, edit2, grades2, meanGrade2, label.
 An example row from this config is {""id"": ""\""10920-9866\"""", ""original1"": ""\""\\\"" Gene Cernan , Last <Astronaut/> on the Moon , ..."", ""edit1"": ""\""Dancer\"""", ""grades1"": ""\""01113\"""", ""meanGrade1"": ""1.2"", ""original2"": ""\""\\\"" Gene Cernan , Last Astronaut on the Moon , <Di..."", ""edit2"": ""\""impregnated\"""", ""grades2"": ""\""30001\"""", ""meanGrade2"": ""0.8"", ""label"": ""1""}.

 




[17] hyperpartisan_news_detection
: Description-Hyperpartisan News Detection was a dataset created for PAN @ SemEval 2019 Task 4.
Given a news article text, decide whether it follows a hyperpartisan argumentation, i.e., whether it exhibits blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person.

There are 2 parts:
- byarticle: Labeled through crowdsourcing on an article basis. The data contains only articles for which a consensus among the crowdsourcing workers existed.
- bypublisher: Labeled by the overall bias of the publisher as provided by BuzzFeed journalists or MediaBiasFactCheck.com.
.
. This dataset has the following configs:
  

	[b] byarticle
: The columns in this config are text, title, hyperpartisan, url, published_at.
 An example row from this config is {""text"": ""\""<p>Money ( <a href=\\\""https://farm8.static.flickr...."", ""title"": ""\""Kucinich: Reclaiming the money power\"""", ""hyperpartisan"": ""true"", ""url"": ""\""https://www.opednews.com/articles/Kucinich-Reclai..."", ""published_at"": ""\""2017-09-10\""""}.

 




[18] kor_hate
: Description-Human-annotated Korean corpus collected from a popular domestic entertainment news aggregation platform
for toxic speech detection. Comments are annotated for gender bias, social bias and hate speech.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are comments, contain_gender_bias, bias, hate.
 An example row from this config is {""comments"": ""\""(\\ud604\\uc7ac \\ud638\\ud154\\uc8fc\\uc778 \\uc2ec\\uc8..."", ""contain_gender_bias"": ""0"", ""bias"": ""2"", ""hate"": ""0""}.

 




[19] TrainingDataPro/high_quality_webcam_video_attacks
: Description-The dataset includes live-recorded Anti-Spoofing videos from around the world,
captured via **high-quality** webcams with Full HD resolution and above. 
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are video_file, assignment_id, worker_id, gender, age, country, resolution.
 An example row from this config is {""video_file"": ""\""0001d815c0--61dd5f24a14d0d2b3b211c83.mp4\"""", ""assignment_id"": ""\""0001d815c0--61dd5f24a14d0d2b3b211c83\"""", ""worker_id"": ""\""a1fa79b217009436a1360b0203509ad4\"""", ""gender"": ""\""FEMALE\"""", ""age"": ""59"", ""country"": ""\""KZ\"""", ""resolution"": ""\""1080.0 x 1920.0\""""}.

 




[20] nbroad/mediasum
: Description-This large-scale media interview dataset contains 463.6K transcripts with abstractive summaries, 
collected from interview transcripts and overview / topic descriptions from NPR and CNN.
.
. This dataset has the following configs:
  

	[b] mediasum
: The columns in this config are id, program, date, url, title, summary, utt, speaker.
 An example row from this config is {""id"": ""\""NPR-1\"""", ""program"": ""\""News & Notes\"""", ""date"": ""\""2007-11-28\"""", ""url"": ""\""https://www.npr.org/templates/story/story.php?sto..."", ""title"": ""\""Black Actors Give Bible Star Appeal\"""", ""summary"": ""\""More than 400 black actors, artists and ministers..."", ""utt"": ""[\""Now, moving on, Forest Whitaker as Moses, Tisha ..."", ""speaker"": ""[\""FARAI CHIDEYA, host\"", \""FARAI CHIDEYA, host\"", \""Mr...""}.

 





The reranking results of the 20 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,,
task1091_ted_translation_en_it," ""You are given a sentence in English. Your job is to translate the English sentence into Italian.""
 {
            ""input"": ""Why? Because that profit allows whatever solution we've created to be infinitely scalable."",
            ""output"": ""Perché? Perché quel profitto fa sì che qualunque soluzione da noi creata sia infinitamente riproducibile su scala."",
            ""explanation"": ""The English sentence is correctly translated into Italian, because the meaning is preserved.""
        },
        {
            ""input"": ""I chose to build there a blessed life."",
            ""output"": ""Ho scelto di costruirmi una vita fortunata."",
            ""explanation"": ""The English sentence is correctly translated into Italian, because the meaning is preserved.""
        },
        {
            ""input"": ""These are really hybrids, not pure animals."",
            ""output"": ""Questi sono ibridi veri, non animali puri."",
            ""explanation"": ""The English sentence is correctly translated into Italian, because the meaning is preserved.""
        }",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 You are given a sentence in English. Your job is to translate the English sentence into Italian. and these are some examples of the same: {
            ""input"": ""Why? Because that profit allows whatever solution we've created to be infinitely scalable."",
            ""output"": ""Perché? Perché quel profitto fa sì che qualunque soluzione da noi creata sia infinitamente riproducibile su scala."",
            ""explanation"": ""The English sentence is correctly translated into Italian, because the meaning is preserved.""
        },
        {
            ""input"": ""I chose to build there a blessed life."",
            ""output"": ""Ho scelto di costruirmi una vita fortunata."",
            ""explanation"": ""The English sentence is correctly translated into Italian, because the meaning is preserved.""
        },
        {
            ""input"": ""These are really hybrids, not pure animals."",
            ""output"": ""Questi sono ibridi veri, non animali puri."",
            ""explanation"": ""The English sentence is correctly translated into Italian, because the meaning is preserved.""
        } 

There are 18 datasets available for this task, each indicated by number identifier []. 

[1] sst2
: Description-The Stanford Sentiment Treebank consists of sentences from movie reviews and
human annotations of their sentiment. The task is to predict the sentiment of a
given sentence. We use the two-way (positive/negative) class split, and use only
sentence-level labels.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are idx, sentence, label.
 An example row from this config is {""idx"": ""0"", ""sentence"": ""\""hide new secretions from the parental units \"""", ""label"": ""0""}.

 




[2] maritaca-ai/sst2_pt
: Description-The Stanford Sentiment Treebank consists of sentences from movie reviews and
human annotations of their sentiment. The task is to predict the sentiment of a
given sentence. We use the two-way (positive/negative) class split, and use only
sentence-level labels.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""esconder novas secre\\u00e7\\u00f5es das unidades p..."", ""label"": ""0""}.

 




[3] sst
: Description-The Stanford Sentiment Treebank, the first corpus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of sentiment in language.
.
. This dataset has the following configs:
  

	[b] ptb
: The columns in this config are ptb_tree.
 An example row from this config is {""ptb_tree"": ""\""(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destin...""}.

 




[4] vesteinn/swe-nerc
: Description-The corpus consists of ca. 150.000 words of text.
.
. This dataset has the following configs:
  

	[b] swe-nerc
: The columns in this config are id, tokens, ner_tags.
 An example row from this config is {""id"": ""\""0\"""", ""tokens"": ""[\""Det\"", \""har\"", \""iaf\"", \""jag\"", \""gjort\"", \""men\"", \""ska\""..."", ""ner_tags"": ""[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...""}.

 




[5] jakartaresearch/inglish
: Description-This dataset is built as a playground for beginner to make a translation model for Indonesian and English.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are english, indonesian.
 An example row from this config is {""english"": ""\""Amrozi accused his brother, whom he called \\\""the ..."", ""indonesian"": ""\""Amrozi menuduh saudaranya, yang dia sebut \\\""saksi...""}.

 




[6] opus_xhosanavy
: Description-This dataset is designed for machine translation from English to Xhosa..
. This dataset has the following configs:
  

	[b] en-xh
: The columns in this config are translation_en, translation_xh.
 An example row from this config is {""translation.en"": ""\""Rope and its Usage\"""", ""translation.xh"": ""\""Intambo nomsebenzi ewenzayo.\""""}.

 




[7] un_multi
: Description-This is a collection of translated documents from the United Nations. This corpus is available in all 6 official languages of the UN, consisting of around 300 million words per language
.
. This dataset has the following configs:
  

	[b] es-zh
: The columns in this config are translation_es, translation_zh.
 An example row from this config is {""translation.es"": ""\""Nueva York, 2 a 27 de mayo de 2005\"""", ""translation.zh"": ""\""2005\\u5e745\\u67082\\u65e5\\u81f327\\u65e5\\uff0c\\u7eb...""}.

 

	[c] ar-fr
: The columns in this config are translation_ar, translation_fr.
 An example row from this config is {""translation.ar"": ""\""\\u0646\\u064a\\u0648\\u064a\\u0648\\u0631\\u0643\\u060c ..."", ""translation.fr"": ""\""New York, 2-27 mai 2005\""""}.

 




[8] opus/liv4ever
: Description-This is the Livonian 4-lingual parallel corpus. Livonian is a Uralic / Finnic language with just about 20 fluent
speakers and no native speakers (as of 2021). The texts and translations in this corpus were collected from all the
digital text resources that could be found by the authors; scanned and printed materials are left for future work.
.
. This dataset has the following configs:
  

	[b] et
: The columns in this config are text.
 An example row from this config is {""text"": ""\""Kus sa l\\u00e4hed, marjaneitsi\""""}.

 

	[c] liv-lv
: The columns in this config are translation_liv, translation_lv.
 An example row from this config is {""translation.liv"": ""\""K\\u00e4dk\\u00f5ks v\\u0113tsi\\u0146\\u021b\\u00f5b t..."", ""translation.lv"": ""\""Ar roku m\\u0101j vi\\u0146am it k\\u0101 uz redz\\u0...""}.

 

	[d] en
: The columns in this config are text.
 An example row from this config is {""text"": ""\""As hydronyms are generally ancient, the names of ...""}.

 

	[e] liv
: The columns in this config are text.
 An example row from this config is {""text"": ""\""Kus sa l\\u01dfd, M\\u014d\\u0157\\u00f5neitst\""""}.

 

	[f] lv
: The columns in this config are text.
 An example row from this config is {""text"": ""\""Valsts Prezidenta prieks\\u030cva\\u0304rds\""""}.

 




[9] multi_nli_mismatch
: Description-The Multi-Genre Natural Language Inference (MultiNLI) corpus is a
crowd-sourced collection of 433k sentence pairs annotated with textual
entailment information. The corpus is modeled on the SNLI corpus, but differs in
that covers a range of genres of spoken and written text, and supports a
distinctive cross-genre generalization evaluation. The corpus served as the
basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are premise, hypothesis, label.
 An example row from this config is {""premise"": ""\""Conceptually cream skimming has two basic dimensi..."", ""hypothesis"": ""\""Product and geography are what make cream skimmin..."", ""label"": ""\""neutral\""""}.

 




[10] multi_nli
: Description-The Multi-Genre Natural Language Inference (MultiNLI) corpus is a
crowd-sourced collection of 433k sentence pairs annotated with textual
entailment information. The corpus is modeled on the SNLI corpus, but differs in
that covers a range of genres of spoken and written text, and supports a
distinctive cross-genre generalization evaluation. The corpus served as the
basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are promptID, pairID, premise, premise_binary_parse, premise_parse, hypothesis, hypothesis_binary_parse, hypothesis_parse, genre, label.
 An example row from this config is {""promptID"": ""31193"", ""pairID"": ""\""31193n\"""", ""premise"": ""\""Conceptually cream skimming has two basic dimensi..."", ""premise_binary_parse"": ""\""( ( Conceptually ( cream skimming ) ) ( ( has ( (..."", ""premise_parse"": ""\""(ROOT (S (NP (JJ Conceptually) (NN cream) (NN ski..."", ""hypothesis"": ""\""Product and geography are what make cream skimmin..."", ""hypothesis_binary_parse"": ""\""( ( ( Product and ) geography ) ( ( are ( what ( ..."", ""hypothesis_parse"": ""\""(ROOT (S (NP (NN Product) (CC and) (NN geography)..."", ""genre"": ""\""government\"""", ""label"": ""1""}.

 




[11] vesteinn/sosialurin-faroese-pos
: Description-The corpus that has been created consists of ca. 100.000 words of text from the [Faroese] newspaper Sosialurin. Each word is tagged with grammatical information (word class, gender, number etc.)
.
. This dataset has the following configs:
  

	[b] sosialurin-faroese-pos
: The columns in this config are id, tokens, pos_tags.
 An example row from this config is {""id"": ""\""0\"""", ""tokens"": ""[\""Gu\\u00f0ri\\u00f0\"", \""Poulsen\"", \""\\u00ed\"", \""Riberh\\..."", ""pos_tags"": ""[277, 327, 111, 318]""}.

 




[12] frtna/jwt300_mt
: Description-This new dataset is designed to be used in the scope of machine translation project.
.
. This dataset has the following configs:
  

	[b] jwt300_mt
: The columns in this config are translation_Spanish, translation_Italian.
 An example row from this config is {""translation.Spanish"": ""\""En este momento se encuentra protegido aproximada..."", ""translation.Italian"": ""\""Al momento circa l \\u2019 8 per cento \\u00e8 prot...""}.

 




[13] opus_euconst
: Description-A parallel corpus collected from the European Constitution for 21 language.
.
. This dataset has the following configs:
  

	[b] fr-hu
: The columns in this config are translation_fr, translation_hu.
 An example row from this config is {""translation.fr"": ""\""Celex Test  \"""", ""translation.hu"": ""\""Celex Test  \""""}.

 

	[c] el-lv
: The columns in this config are translation_el, translation_lv.
 An example row from this config is {""translation.el"": ""\""Celex Test  \"""", ""translation.lv"": ""\""Celex Test  \""""}.

 

	[d] cs-pl
: The columns in this config are translation_cs, translation_pl.
 An example row from this config is {""translation.cs"": ""\""Celex Test  \"""", ""translation.pl"": ""\""Celex Test  \""""}.

 

	[e] pt-sv
: The columns in this config are translation_pt, translation_sv.
 An example row from this config is {""translation.pt"": ""\""Celex Test  \"""", ""translation.sv"": ""\""Celex Test  \""""}.

 

	[f] fr-pt
: The columns in this config are translation_fr, translation_pt.
 An example row from this config is {""translation.fr"": ""\""Celex Test  \"""", ""translation.pt"": ""\""Celex Test  \""""}.

 




[14] snli
: Description-The SNLI corpus (version 1.0) is a collection of 570k human-written English
sentence pairs manually labeled for balanced classification with the labels
entailment, contradiction, and neutral, supporting the task of natural language
inference (NLI), also known as recognizing textual entailment (RTE).
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are premise, hypothesis, label.
 An example row from this config is {""premise"": ""\""A person on a horse jumps over a broken down airp..."", ""hypothesis"": ""\""A person is training his horse for a competition...."", ""label"": ""1""}.

 




[15] shibing624/snli-zh
: Description-The SNLI corpus (version 1.0) is a collection of 570k human-written English
sentence pairs manually labeled for balanced classification with the labels
entailment, contradiction, and neutral, supporting the task of natural language
inference (NLI), also known as recognizing textual entailment (RTE).
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are premise, hypothesis, label.
 An example row from this config is {""premise"": ""\""\\u662f\\u7684\\uff0c\\u6211\\u60f3\\u4e00\\u4e2a\\u6d1e\\..."", ""hypothesis"": ""\""\\u6211\\u8ba4\\u4e3a\\u6d1e\\u7a74\\u53ef\\u80fd\\u4f1a\\..."", ""label"": ""1""}.

 




[16] persiannlp/parsinlu_sentiment
: Description-A Persian sentiment analysis task (deciding whether a given sentence contains a particular sentiment).     
.
. This dataset has the following configs:
  

	[b] parsinlu-repo
: The columns in this config are review, review_id, example_id, excel_id, question, category, aspect, label, guid.
 An example row from this config is {""review"": ""\""\\u062f\\u0648\\u0633\\u062a\\u0627\\u0646 \\u062d\\u062a..."", ""review_id"": ""\""1\"""", ""example_id"": ""\""1\"""", ""excel_id"": ""\""food_1744\"""", ""question"": ""\""\\u0646\\u0638\\u0631 \\u0634\\u0645\\u0627 \\u062f\\u063..."", ""category"": ""\""\\u06af\\u0648\\u0634\\u062a \\u0645\\u0631\\u063a\"""", ""aspect"": ""\""\\u0637\\u0639\\u0645\"""", ""label"": ""\""-3\"""", ""guid"": ""\""food-train-r1-e1\""""}.

 




[17] ecb
: Description-Original source: Website and documentatuion from the European Central Bank, compiled and made available by Alberto Simoes (thank you very much!)
19 languages, 170 bitexts
total number of files: 340
total number of tokens: 757.37M
total number of sentence fragments: 30.55M
.
. This dataset has the following configs:
  

	[b] de-fr
: The columns in this config are id, translation_de, translation_fr.
 An example row from this config is {""id"": ""\""0\"""", ""translation.de"": ""\""Navigation Path : Home &gt; The European Central ..."", ""translation.fr"": ""\""Navigation Path : Home &gt; The European Central ...""}.

 

	[c] cs-en
: The columns in this config are id, translation_cs, translation_en.
 An example row from this config is {""id"": ""\""0\"""", ""translation.cs"": ""\""Navigation Path : Home &gt; The European Central ..."", ""translation.en"": ""\""Navigation Path : Home &gt; The European Central ...""}.

 

	[d] el-it
: The columns in this config are id, translation_el, translation_it.
 An example row from this config is {""id"": ""\""0\"""", ""translation.el"": ""\""EL\"""", ""translation.it"": ""\""IT\""""}.

 

	[e] fi-pl
: The columns in this config are id, translation_fi, translation_pl.
 An example row from this config is {""id"": ""\""0\"""", ""translation.fi"": ""\""Py\\u00f6ristyksist\\u00e4 johtuen yhteenlaskut eiv..."", ""translation.pl"": ""\""Poszczeg\\u00f3lne pozycje mog\\u0105 nie sumowa\\u0...""}.

 




[18] pragmeval
: Description-Evaluation of language understanding with a 11 datasets benchmark focusing on discourse and pragmatics
.
. This dataset has the following configs:
  

	[b] verifiability
: The columns in this config are sentence, label, idx.
 An example row from this config is {""sentence"": ""\""dot's objective in setting this rule should be to..."", ""label"": ""1"", ""idx"": ""0""}.

 

	[c] switchboard
: The columns in this config are sentence, label, idx.
 An example row from this config is {""sentence"": ""\""i had thought that they had, uh, colonies there, ..."", ""label"": ""28"", ""idx"": ""0""}.

 

	[d] sarcasm
: The columns in this config are sentence1, sentence2, label, idx.
 An example row from this config is {""sentence1"": ""\""it is a good thing. but why must we destroy the e..."", ""sentence2"": ""\""\\\""what is this \\\""\\\""we\\\""\\\"" stuff? got a mouse in y..."", ""label"": ""1"", ""idx"": ""0""}.

 

	[e] emobank-valence
: The columns in this config are sentence, label, idx.
 An example row from this config is {""sentence"": ""\""\\\""now leave, before i call the rats on you.\\\""\\\""\\\""..."", ""label"": ""0"", ""idx"": ""0""}.

 

	[f] pdtb
: The columns in this config are sentence1, sentence2, label, idx.
 An example row from this config is {""sentence1"": ""\""when stock prices moved broadly lower\"""", ""sentence2"": ""\""the dow jones industrial average fell 26.23 point..."", ""label"": ""7"", ""idx"": ""0""}.

 





The reranking results of the 18 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,,
task1403_check_validity_date_mmddyyyy,"""In this task, you are given a date in \""mm/dd/yyyy\"" format. You need to check if the date is valid or not. Return 1 if it is valid, else return 0. A date is valid is the components month(\""mm\""), day(\""dd\"") and year(\""yyyy\"") are all valid individually. A day(dd) is valid if it is greater than or equal to 1 and less than 30 or 31 depending upon the month(mm). Months which have 31 days are January, March, May, July, August, October, December. Rest of the months have 30 days except February which has 28 days if it is not a leap year and 29 days if it is a leap year. A month(mm) is valid if it lies in the range from 1 to 12 as there are 12 months in a year. A year is always valid if it is expressed in the form of \""yyyy\"".""
        
""input"": ""14/25/1405"",
""output"": ""0"",
""explanation"": ""It is an invalid date as the month(mm) is 14 which does not lie in the range 1 to 12.""

""input"": ""07/29/1617"",
""output"": ""1"",
""explanation"": ""It is a valid date as month(mm), day(dd) and year(yyyy) are all valid.""",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 ""In this task, you are given a date in \""mm/dd/yyyy\"" format. You need to check if the date is valid or not. Return 1 if it is valid, else return 0. A date is valid is the components month(\""mm\""), day(\""dd\"") and year(\""yyyy\"") are all valid individually. A day(dd) is valid if it is greater than or equal to 1 and less than 30 or 31 depending upon the month(mm). Months which have 31 days are January, March, May, July, August, October, December. Rest of the months have 30 days except February which has 28 days if it is not a leap year and 29 days if it is a leap year. A month(mm) is valid if it lies in the range from 1 to 12 as there are 12 months in a year. A year is always valid if it is expressed in the form of \""yyyy\""."" and these are some examples of the same: ""input"": ""14/25/1405"",
""output"": ""0"",
""explanation"": ""It is an invalid date as the month(mm) is 14 which does not lie in the range 1 to 12.""

""input"": ""07/29/1617"",
""output"": ""1"",
""explanation"": ""It is a valid date as month(mm), day(dd) and year(yyyy) are all valid."" 

There are 10 datasets available for this task, each indicated by number identifier []. 

[1] open_subtitles
: Description-This is a new collection of translated movie subtitles from http://www.opensubtitles.org/.

IMPORTANT: If you use the OpenSubtitle corpus: Please, add a link to http://www.opensubtitles.org/ to your website and to your reports and publications produced with the data!

This is a slightly cleaner version of the subtitle collection using improved sentence alignment and better language checking.

62 languages, 1,782 bitexts
total number of files: 3,735,070
total number of tokens: 22.10G
total number of sentence fragments: 3.35G
.
. This dataset has the following configs:
  

	[b] bs-eo
: The columns in this config are id, meta_year, meta_imdbId, meta_subtitleId_bs, meta_subtitleId_eo, meta_sentenceIds_bs, meta_sentenceIds_eo, translation_bs, translation_eo.
 An example row from this config is {""id"": ""\""0\"""", ""meta.year"": ""1973"", ""meta.imdbId"": ""70215"", ""meta.subtitleId.bs"": ""6080330"", ""meta.subtitleId.eo"": ""4010963"", ""meta.sentenceIds.bs"": ""[1]"", ""meta.sentenceIds.eo"": ""[2]"", ""translation.bs"": ""\""Gospodine Borgard...\"""", ""translation.eo"": ""\""Alvenis la respondo por vi el Nov-Orleano.\""""}.

 

	[c] fr-hy
: The columns in this config are id, meta_year, meta_imdbId, meta_subtitleId_fr, meta_subtitleId_hy, meta_sentenceIds_fr, meta_sentenceIds_hy, translation_fr, translation_hy.
 An example row from this config is {""id"": ""\""0\"""", ""meta.year"": ""1971"", ""meta.imdbId"": ""67372"", ""meta.subtitleId.fr"": ""3693493"", ""meta.subtitleId.hy"": ""6711716"", ""meta.sentenceIds.fr"": ""[1]"", ""meta.sentenceIds.hy"": ""[1]"", ""translation.fr"": ""\""A quand rendez-vous prochain ?\"""", ""translation.hy"": ""\""\\u054e\\u0565\\u0570\\u0568 \\u0566\\u0561\\u0566\\u0580...""}.

 

	[d] da-ru
: The columns in this config are id, meta_year, meta_imdbId, meta_subtitleId_da, meta_subtitleId_ru, meta_sentenceIds_da, meta_sentenceIds_ru, translation_da, translation_ru.
 An example row from this config is {""id"": ""\""0\"""", ""meta.year"": ""1927"", ""meta.imdbId"": ""17136"", ""meta.subtitleId.da"": ""61728"", ""meta.subtitleId.ru"": ""42690"", ""meta.sentenceIds.da"": ""[1, 2]"", ""meta.sentenceIds.ru"": ""[1]"", ""translation.da"": ""\""Hver epoke skaber sin efterf\\u00f8lger - Jules Mi..."", ""translation.ru"": ""\""\\u041a\\u0430\\u0436\\u0434\\u0430\\u044f \\u044d\\u043f...""}.

 

	[e] en-hi
: The columns in this config are id, meta_year, meta_imdbId, meta_subtitleId_en, meta_subtitleId_hi, meta_sentenceIds_en, meta_sentenceIds_hi, translation_en, translation_hi.
 An example row from this config is {""id"": ""\""0\"""", ""meta.year"": ""1948"", ""meta.imdbId"": ""40522"", ""meta.subtitleId.en"": ""4180294"", ""meta.subtitleId.hi"": ""4239106"", ""meta.sentenceIds.en"": ""[1]"", ""meta.sentenceIds.hi"": ""[1]"", ""translation.en"": ""\""THE BICYCLE THIEF\"""", ""translation.hi"": ""\""\\u0938\\u093e\\u0907\\u0915\\u093f\\u0932 \\u091a\\u094b...""}.

 

	[f] bn-is
: The columns in this config are id, meta_year, meta_imdbId, meta_subtitleId_bn, meta_subtitleId_is, meta_sentenceIds_bn, meta_sentenceIds_is, translation_bn, translation_is.
 An example row from this config is {""id"": ""\""0\"""", ""meta.year"": ""1981"", ""meta.imdbId"": ""82971"", ""meta.subtitleId.bn"": ""6443778"", ""meta.subtitleId.is"": ""4634729"", ""meta.sentenceIds.bn"": ""[2]"", ""meta.sentenceIds.is"": ""[2]"", ""translation.bn"": ""\""\\u09b9\\u09ac\\u09bf\\u099f\\u09cb\\u09b8 \\u0995\\u09be..."", ""translation.is"": ""\""Eitri\\u00f0 er enn \\u00f6flugt.\""""}.

 




[2] dbpedia_14
: Description-The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes
from DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we
randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size
of the training dataset is 560,000 and testing dataset 70,000.
There are 3 columns in the dataset (same for train and test splits), corresponding to class index
(1 to 14), title and content. The title and content are escaped using double quotes (""), and any
internal double quote is escaped by 2 double quotes (""""). There are no new lines in title or content.
.
. This dataset has the following configs:
  

	[b] dbpedia_14
: The columns in this config are label, title, content.
 An example row from this config is {""label"": ""0"", ""title"": ""\""E. D. Abbott Ltd\"""", ""content"": ""\"" Abbott of Farnham E D Abbott Limited was a Briti...""}.

 




[3] RohanAiLab/persian_daily_news
: Description-Persian Daily News dataset is a collection of 2 million news articles with the headline of each news article.
This dataset contains news articles and their summaries for the last 10 years.
This dataset is provided by Rohan AI lab for research purposes.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, summary.
 An example row from this config is {""text"": ""\""\\u0628\\u0647 \\u06af\\u0632\\u0627\\u0631\\u0634 \\u062..."", ""summary"": ""\""\\u0633\\u062e\\u0646\\u06af\\u0648\\u06cc \\u0648\\u0632...""}.

 




[4] opus_openoffice
: Description-A collection of documents from http://www.openoffice.org/.
.
. This dataset has the following configs:
  

	[b] ru-zh_CN
: The columns in this config are translation_ru, translation_zh_CN.
 An example row from this config is {""translation.ru"": ""\""\\u0414\\u0438\\u0430\\u0433\\u0440\\u0430\\u043c\\u043c\\..."", ""translation.zh_CN"": ""\""$[officename] \\u4e2d\\u7684\\u56fe\\u8868\""""}.

 

	[c] sv-zh_CN
: The columns in this config are translation_sv, translation_zh_CN.
 An example row from this config is {""translation.sv"": ""\""Diagram i $[officename]\"""", ""translation.zh_CN"": ""\""$[officename] \\u4e2d\\u7684\\u56fe\\u8868\""""}.

 

	[d] en_GB-ru
: The columns in this config are translation_en_GB, translation_ru.
 An example row from this config is {""translation.en_GB"": ""\""Charts in $[officename]\"""", ""translation.ru"": ""\""\\u0414\\u0438\\u0430\\u0433\\u0440\\u0430\\u043c\\u043c\\...""}.

 

	[e] en_GB-es
: The columns in this config are translation_en_GB, translation_es.
 An example row from this config is {""translation.en_GB"": ""\""Charts in $[officename]\"""", ""translation.es"": ""\""Gr\\u00e1ficas en $[officename]\""""}.

 

	[f] ja-sv
: The columns in this config are translation_ja, translation_sv.
 An example row from this config is {""translation.ja"": ""\""$[officename] \\u306e\\u30b0\\u30e9\\u30d5\"""", ""translation.sv"": ""\""Diagram i $[officename]\""""}.

 




[5] RohanAiLab/persian_news_dataset
: Description-
persian_news_dataset is a collection of 5 million news articles. 
News articles have been gathered from more than 10 news agencies for the last 12 years. 
The dataset is provided by Rohan AI lab for research purposes.
for more information refer to this link:
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, title, category.
 An example row from this config is {""text"": ""\""\\u0627\\u0647\\u0648\\u0627\\u0632- \\u0633\\u062e\\u064..."", ""title"": ""\""\"""", ""category"": ""\""\""""}.

 




[6] blog_authorship_corpus
: Description-The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.

Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)

All bloggers included in the corpus fall into one of three age groups:
- 8240 ""10s"" blogs (ages 13-17),
- 8086 ""20s"" blogs (ages 23-27),
- 2994 ""30s"" blogs (ages 33-47).

For each age group there are an equal number of male and female bloggers.

Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.

The corpus may be freely used for non-commercial research purposes.
.
. This dataset has the following configs:
  

	[b] blog_authorship_corpus
: The columns in this config are text, date, gender, age, horoscope, job.
 An example row from this config is {""text"": ""\""Yeah, sorry for not writing for a whole there, bu..."", ""date"": ""\""23,November,2002\"""", ""gender"": ""\""female\"""", ""age"": ""17"", ""horoscope"": ""\""Libra\"""", ""job"": ""\""Student\""""}.

 




[7] enwik8
: Description-The dataset is based on the Hutter Prize (http://prize.hutter1.net) and contains the first 10^8 bytes of English Wikipedia in 2006 in XML
.
. This dataset has the following configs:
  

	[b] enwik8
: The columns in this config are text.
 An example row from this config is {""text"": ""\""<mediawiki xmlns=\\\""http://www.mediawiki.org/xml/e...""}.

 

	[c] enwik8-raw
: The columns in this config are text.
 An example row from this config is {""text"": ""\""<mediawiki xmlns=\\\""http://www.mediawiki.org/xml/e...""}.

 




[8] vesteinn/swe-nerc
: Description-The corpus consists of ca. 150.000 words of text.
.
. This dataset has the following configs:
  

	[b] swe-nerc
: The columns in this config are id, tokens, ner_tags.
 An example row from this config is {""id"": ""\""0\"""", ""tokens"": ""[\""Det\"", \""har\"", \""iaf\"", \""jag\"", \""gjort\"", \""men\"", \""ska\""..."", ""ner_tags"": ""[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...""}.

 




[9] code_x_glue_tt_text_to_text
: Description-CodeXGLUE text-to-text dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Text-Text/text-to-text

The dataset we use is crawled and filtered from Microsoft Documentation, whose document located at https://github.com/MicrosoftDocs/..
. This dataset has the following configs:
  

	[b] da_en
: The columns in this config are id, source, target.
 An example row from this config is {""id"": ""0"", ""source"": ""\""title : &quot; Oversigt over ops\\u00e6tninger for..."", ""target"": ""\""title : Overview of Setups for Service Items and ...""}.

 

	[c] lv_en
: The columns in this config are id, source, target.
 An example row from this config is {""id"": ""0"", ""source"": ""\""title : Pakalpojumu objektu izveide\\n\"""", ""target"": ""\""title : Create service objects\\n\""""}.

 

	[d] no_en
: The columns in this config are id, source, target.
 An example row from this config is {""id"": ""0"", ""source"": ""\""title : Oversikt over oppsett av servicevarer og ..."", ""target"": ""\""title : Overview of Setups for Service Items and ...""}.

 

	[e] zh_en
: The columns in this config are id, source, target.
 An example row from this config is {""id"": ""0"", ""source"": ""\""\\u4ee5\\u4e0b \\u547d\\u540d \\u7a7a\\u95f4 \\u5305\\u54..."", ""target"": ""\""The following namespaces contain APIs that allow ...""}.

 




[10] un_multi
: Description-This is a collection of translated documents from the United Nations. This corpus is available in all 6 official languages of the UN, consisting of around 300 million words per language
.
. This dataset has the following configs:
  

	[b] de-zh
: The columns in this config are translation_de, translation_zh.
 An example row from this config is {""translation.de"": ""\""Sechzigste Tagung\"""", ""translation.zh"": ""\""\\u5927\\u4f1a\\u51b3\\u8bae\""""}.

 

	[c] ar-es
: The columns in this config are translation_ar, translation_es.
 An example row from this config is {""translation.ar"": ""\""\\u0646\\u064a\\u0648\\u064a\\u0648\\u0631\\u0643\\u060c ..."", ""translation.es"": ""\""Nueva York, 2 a 27 de mayo de 2005\""""}.

 

	[d] de-en
: The columns in this config are translation_de, translation_en.
 An example row from this config is {""translation.de"": ""\""Resolution der Generalversammlung\"""", ""translation.en"": ""\""Resolution adopted by the General Assembly\""""}.

 

	[e] de-es
: The columns in this config are translation_de, translation_es.
 An example row from this config is {""translation.de"": ""\""Resolution der Generalversammlung\"""", ""translation.es"": ""\""Resoluci\\u00f3n aprobada por la Asamblea General\""...""}.

 

	[f] es-zh
: The columns in this config are translation_es, translation_zh.
 An example row from this config is {""translation.es"": ""\""Nueva York, 2 a 27 de mayo de 2005\"""", ""translation.zh"": ""\""2005\\u5e745\\u67082\\u65e5\\u81f327\\u65e5\\uff0c\\u7eb...""}.

 





The reranking results of the 10 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,,
task1343_amazon_us_reviews_rating,"Given an Amazon review, indicate whether it is a 'Positive Review' or 'Negative Review'.
{""input"": ""Bought cables in 3ft, 6ft and 9ft.  NONE of them worked.  NO FUNCTIONALITY WHATSOEVER.  Tested many times, its as if the copper wires are just not connected to the terminations.  Do these even go through Quality Control before they leave the factory?  Waste of money and time."",
""output"": ""Negative Review"",
 ""explanation"": ""User did not like cables at all and found all of them useless so it is a negative review.""
}",,"Your objective is to rerank datasets given a task (and few examples of the task) based on relevancy. Relevant factors involve how suited the dataset is for the task, whether the input/output formats of the task and the dataset match.  Each dataset is indicated by number identifier []. For each dataset, you will be provided with the dataset description, and the configurations available for each dataset. Each config will be represented with the config name, the columns in the dataset, and a few example rows. Please return the combination of the most relevant dataset, with the best suited configs using their identifiers and name, with the most relevant passages listed first, along with a confidence level ranging from [low, medium, high] . The output format should be a tuple of form (dataset_index,dataset_name,config_index,config_name,confidence_level). e.g., (1,squad,a,""plain_text"",low). 

------

The following is the task 
 Given an Amazon review, indicate whether it is a 'Positive Review' or 'Negative Review'. and these are some examples of the same: N/A 

There are 11 datasets available for this task, each indicated by number identifier []. 

[1] amazon_polarity
: Description-The Amazon reviews dataset consists of reviews from amazon.
The data span a period of 18 years, including ~35 million reviews up to March 2013.
Reviews include product and user information, ratings, and a plaintext review.
.
. This dataset has the following configs:
  

	[b] amazon_polarity
: The columns in this config are label, title, content.
 An example row from this config is {""label"": ""1"", ""title"": ""\""Stuning even for the non-gamer\"""", ""content"": ""\""This sound track was beautiful! It paints the sen...""}.

 




[2] sst2
: Description-The Stanford Sentiment Treebank consists of sentences from movie reviews and
human annotations of their sentiment. The task is to predict the sentiment of a
given sentence. We use the two-way (positive/negative) class split, and use only
sentence-level labels.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are idx, sentence, label.
 An example row from this config is {""idx"": ""0"", ""sentence"": ""\""hide new secretions from the parental units \"""", ""label"": ""0""}.

 




[3] maritaca-ai/sst2_pt
: Description-The Stanford Sentiment Treebank consists of sentences from movie reviews and
human annotations of their sentiment. The task is to predict the sentiment of a
given sentence. We use the two-way (positive/negative) class split, and use only
sentence-level labels.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""esconder novas secre\\u00e7\\u00f5es das unidades p..."", ""label"": ""0""}.

 




[4] wiki_qa
: Description-Wiki Question Answering corpus from Microsoft
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are question_id, question, document_title, answer, label.
 An example row from this config is {""question_id"": ""\""Q1\"""", ""question"": ""\""how are glacier caves formed?\"""", ""document_title"": ""\""Glacier cave\"""", ""answer"": ""\""A partly submerged glacier cave on Perito Moreno ..."", ""label"": ""0""}.

 




[5] turkish_product_reviews
: Description-
Turkish Product Reviews.
This repository contains 235.165 product reviews collected online. There are 220.284 positive, 14881 negative reviews.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are sentence, sentiment.
 An example row from this config is {""sentence"": ""\""fena de\\u011fil paraya g\\u00f6re iyi.\"""", ""sentiment"": ""1""}.

 




[6] bigbio/scicite
: Description-SciCite is a dataset of 11K manually annotated citation intents based on
citation context in the computer science and biomedical domains.
.
. This dataset has the following configs:
  

	[b] scicite_source
: The columns in this config are source, citeStart, sectionName, string, citeEnd, label, label_confidence, label2, label2_confidence, citingPaperId, citedPaperId, isKeyCitation, id, unique_id, excerpt_index.
 An example row from this config is {""source"": ""\""explicit\"""", ""citeStart"": ""168"", ""sectionName"": ""\""Introduction\"""", ""string"": ""\""However, how frataxin interacts with the Fe-S clu..."", ""citeEnd"": ""175"", ""label"": ""1"", ""label_confidence"": ""1.0"", ""label2"": ""3"", ""label2_confidence"": ""NaN"", ""citingPaperId"": ""\""1872080baa7d30ec8fb87be9a65358cd3a7fb649\"""", ""citedPaperId"": ""\""894be9b4ea46a5c422e81ef3c241072d4c73fdc0\"""", ""isKeyCitation"": ""true"", ""id"": ""\""1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be9b4..."", ""unique_id"": ""\""1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be9b4..."", ""excerpt_index"": ""11""}.

 

	[c] scicite_bigbio_text
: The columns in this config are id, document_id, text, labels.
 An example row from this config is {""id"": ""\""1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be9b4..."", ""document_id"": ""\""1872080baa7d30ec8fb87be9a65358cd3a7fb649\"""", ""text"": ""\""However, how frataxin interacts with the Fe-S clu..."", ""labels"": ""[\""background\""]""}.

 




[7] allegro_reviews
: Description-Allegro Reviews is a sentiment analysis dataset, consisting of 11,588 product reviews written in Polish and extracted
from Allegro.pl - a popular e-commerce marketplace. Each review contains at least 50 words and has a rating on a scale
from one (negative review) to five (positive review).

We recommend using the provided train/dev/test split. The ratings for the test set reviews are kept hidden.
You can evaluate your model using the online evaluation tool available on klejbenchmark.com.
.
. This dataset has the following configs:
  

	[b] default
: The columns in this config are text, rating.
 An example row from this config is {""text"": ""\""Jako do ceny dobra. Przyssawka mog\\u0142aby by\\u0..."", ""rating"": ""3.0""}.

 




[8] humicroedit
: Description-This new dataset is designed to assess the funniness of edited news headlines.
.
. This dataset has the following configs:
  

	[b] subtask-1
: The columns in this config are id, original, edit, grades, meanGrade.
 An example row from this config is {""id"": ""\""14530\"""", ""original"": ""\""France is \\u2018 hunting down its citizens who jo..."", ""edit"": ""\""twins\"""", ""grades"": ""\""10000\"""", ""meanGrade"": ""0.2""}.

 

	[c] subtask-2
: The columns in this config are id, original1, edit1, grades1, meanGrade1, original2, edit2, grades2, meanGrade2, label.
 An example row from this config is {""id"": ""\""10920-9866\"""", ""original1"": ""\""\\\"" Gene Cernan , Last <Astronaut/> on the Moon , ..."", ""edit1"": ""\""Dancer\"""", ""grades1"": ""\""01113\"""", ""meanGrade1"": ""1.2"", ""original2"": ""\""\\\"" Gene Cernan , Last Astronaut on the Moon , <Di..."", ""edit2"": ""\""impregnated\"""", ""grades2"": ""\""30001\"""", ""meanGrade2"": ""0.8"", ""label"": ""1""}.

 




[9] dbrd
: Description-The Dutch Book Review Dataset (DBRD) contains over 110k book reviews of which 22k have associated binary sentiment polarity labels. It is intended as a benchmark for sentiment classification in Dutch and created due to a lack of annotated datasets in Dutch that are suitable for this task.
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are text, label.
 An example row from this config is {""text"": ""\""Na alle voorgaande boeken van Dan Brown gelezen t..."", ""label"": ""1""}.

 




[10] snli
: Description-The SNLI corpus (version 1.0) is a collection of 570k human-written English
sentence pairs manually labeled for balanced classification with the labels
entailment, contradiction, and neutral, supporting the task of natural language
inference (NLI), also known as recognizing textual entailment (RTE).
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are premise, hypothesis, label.
 An example row from this config is {""premise"": ""\""A person on a horse jumps over a broken down airp..."", ""hypothesis"": ""\""A person is training his horse for a competition...."", ""label"": ""1""}.

 




[11] shibing624/snli-zh
: Description-The SNLI corpus (version 1.0) is a collection of 570k human-written English
sentence pairs manually labeled for balanced classification with the labels
entailment, contradiction, and neutral, supporting the task of natural language
inference (NLI), also known as recognizing textual entailment (RTE).
.
. This dataset has the following configs:
  

	[b] plain_text
: The columns in this config are premise, hypothesis, label.
 An example row from this config is {""premise"": ""\""\\u662f\\u7684\\uff0c\\u6211\\u60f3\\u4e00\\u4e2a\\u6d1e\\..."", ""hypothesis"": ""\""\\u6211\\u8ba4\\u4e3a\\u6d1e\\u7a74\\u53ef\\u80fd\\u4f1a\\..."", ""label"": ""1""}.

 





The reranking results of the 11 datasets in (dataset_index,dataset_name,config_index,config_name,confidence_level) format is:",,,amazon_polarity,amazon_polarity
